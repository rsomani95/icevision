{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IceVision: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you Why IceVision? IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly Quick Example: How to train the PETS Dataset Source Code Happy Learning! If you need any assistance, feel free to: Join our Forum","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"IceVision: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you","title":""},{"location":"#why-icevision","text":"IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly","title":"Why IceVision?"},{"location":"#quick-example-how-to-train-the-pets-dataset","text":"Source Code","title":"Quick Example: How to train the PETS Dataset"},{"location":"#happy-learning","text":"If you need any assistance, feel free to: Join our Forum","title":"Happy Learning!"},{"location":"IceApp_coco/","text":"IceVision Deployment App: COCO Dataset This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"COCO"},{"location":"IceApp_coco/#icevision-deployment-app-coco-dataset","text":"This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  COCO Dataset"},{"location":"IceApp_coco/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_coco/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_coco/#loading-trained-model","text":"class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_coco/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_coco/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_coco/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_coco/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_masks/","text":"IceVision Deployment App: PennFudan Dataset This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0] Defining the get_masks method: called by gr.Interface(fn=get_masks, ...) def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface # Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"Masks"},{"location":"IceApp_masks/#icevision-deployment-app-pennfudan-dataset","text":"This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  PennFudan Dataset"},{"location":"IceApp_masks/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_masks/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_masks/#loading-trained-model","text":"class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_masks/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_masks/#defining-the-get_masks-method-called-by-grinterfacefnget_masks","text":"def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the get_masks method: called by gr.Interface(fn=get_masks, ...)"},{"location":"IceApp_masks/#gradio-user-interface","text":"# Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_masks/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_pets/","text":"IceVision Deployment App: PETS Dataset This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"PETS"},{"location":"IceApp_pets/#icevision-deployment-app-pets-dataset","text":"This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App: PETS Dataset"},{"location":"IceApp_pets/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_pets/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_pets/#loading-trained-model","text":"class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_pets/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_pets/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_pets/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_pets/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"albumentations/","text":"Transforms Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases. Usage In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Albumentations"},{"location":"albumentations/#transforms","text":"Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases.","title":"Transforms"},{"location":"albumentations/#usage","text":"In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Usage"},{"location":"albumentations_tfms/","text":"[source] aug_tfms icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 45 , 45 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 20 , 20 ), g_shift_limit = ( - 20 , 20 ), b_shift_limit = ( - 20 , 20 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (height, width). presizing : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source] Adapter icevision . tfms . albumentations . Adapter ( tfms ) Adapter that enables the use of albumentations transforms. Arguments tfms Sequence[albumentations.core.transforms_interface.BasicTransform] : Sequence of albumentation transforms.","title":"Albumentations"},{"location":"albumentations_tfms/#aug_tfms","text":"icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 45 , 45 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 20 , 20 ), g_shift_limit = ( - 20 , 20 ), b_shift_limit = ( - 20 , 20 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (height, width). presizing : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source]","title":"aug_tfms"},{"location":"albumentations_tfms/#adapter","text":"icevision . tfms . albumentations . Adapter ( tfms ) Adapter that enables the use of albumentations transforms. Arguments tfms Sequence[albumentations.core.transforms_interface.BasicTransform] : Sequence of albumentation transforms.","title":"Adapter"},{"location":"backbones_effecientdet/","text":"EffecientDet Backbones Source Usage We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. We can choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) EffecientDet Backbones Examples tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) Supported Backbones EffecientDet Backbones tf_efficientdet_lite0 efficientdet_d0 efficientdet_d1 efficientdet_d2 efficientdet_d3 efficientdet_d4 efficientdet_d5 efficientdet_d6 efficientdet_d7 efficientdet_d7x MobileNetv3 MNasNet MobileNetV2 MixNet","title":"EffecientDet"},{"location":"backbones_effecientdet/#effecientdet-backbones","text":"Source","title":"EffecientDet Backbones"},{"location":"backbones_effecientdet/#usage","text":"We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. We can choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more)","title":"Usage"},{"location":"backbones_effecientdet/#effecientdet-backbones-examples","text":"tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size )","title":"EffecientDet Backbones Examples"},{"location":"backbones_effecientdet/#supported-backbones","text":"EffecientDet Backbones tf_efficientdet_lite0 efficientdet_d0 efficientdet_d1 efficientdet_d2 efficientdet_d3 efficientdet_d4 efficientdet_d5 efficientdet_d6 efficientdet_d7 efficientdet_d7x MobileNetv3 MNasNet MobileNetV2 MixNet","title":"Supported Backbones"},{"location":"backbones_faster_mask_rcnn/","text":"Faster RCNN / Mask RCNN Backbones Source Usage We use the torchvision Faster RCNN model, and the torchvision Mask RCNN model. Both models accept a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the many backbones listed here below: Faster RCNN Backbones Examples fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet18 Example: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) Mask RCNN Backbones Examples fasterrcnn_resnet50_fpn Example: - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet34 Example: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) Supported Backbones FPN backbones - resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 Resnet backbone - resnet18 resnet34 resnet50 resnet101 resnet152 resnext101_32x8d MobileNet - mobilenet VGG vgg11 vgg13 vgg16 vgg19","title":"Faster/Mask RCNN"},{"location":"backbones_faster_mask_rcnn/#faster-rcnn-mask-rcnn-backbones","text":"Source","title":"Faster RCNN / Mask RCNN Backbones"},{"location":"backbones_faster_mask_rcnn/#usage","text":"We use the torchvision Faster RCNN model, and the torchvision Mask RCNN model. Both models accept a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the many backbones listed here below:","title":"Usage"},{"location":"backbones_faster_mask_rcnn/#faster-rcnn-backbones-examples","text":"fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet18 Example: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) )","title":"Faster RCNN Backbones Examples"},{"location":"backbones_faster_mask_rcnn/#mask-rcnn-backbones-examples","text":"fasterrcnn_resnet50_fpn Example: - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet34 Example: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) )","title":"Mask RCNN Backbones Examples"},{"location":"backbones_faster_mask_rcnn/#supported-backbones","text":"FPN backbones - resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 Resnet backbone - resnet18 resnet34 resnet50 resnet101 resnet152 resnext101_32x8d MobileNet - mobilenet VGG vgg11 vgg13 vgg16 vgg19","title":"Supported Backbones"},{"location":"changing_the_colors/","text":"Changing the colors If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder Color scheme Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) }) Primary color The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accent color The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Changing the colors"},{"location":"changing_the_colors/#changing-the-colors","text":"If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder","title":"Changing the colors"},{"location":"changing_the_colors/#color-scheme","text":"Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) })","title":"Color scheme"},{"location":"changing_the_colors/#primary-color","text":"The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) })","title":"Primary color"},{"location":"changing_the_colors/#accent-color","text":"The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Accent color"},{"location":"coco_keypoints/","text":"COCO Keypoints Simple example on how to parse keypoints for the coco annotation format. For demonstration purposes we will be using the samples present on the repo instead of the full COCO dataset. from icevision.all import * data_dir = Path ( '/home/lgvaz/git/icevision/samples' ) class_map = ClassMap ([ 'person' ]) parser = parsers . COCOKeyPointsParser ( annotations_filepath = data_dir / 'keypoints_annotations.json' , img_dir = data_dir / 'images' ) records = parser . parse ( data_splitter = SingleSplitSplitter ())[ 0 ] record = records [ 1 ] show_record ( record , figsize = ( 10 , 10 ), class_map = class_map ) test_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 512 ), tfms . A . Normalize ()]) test_ds = Dataset ( records , test_tfms ) show_sample ( test_ds [ 0 ], figsize = ( 10 , 10 ), display_bbox = False )","title":"COCO Keypoints"},{"location":"coco_keypoints/#coco-keypoints","text":"Simple example on how to parse keypoints for the coco annotation format. For demonstration purposes we will be using the samples present on the repo instead of the full COCO dataset. from icevision.all import * data_dir = Path ( '/home/lgvaz/git/icevision/samples' ) class_map = ClassMap ([ 'person' ]) parser = parsers . COCOKeyPointsParser ( annotations_filepath = data_dir / 'keypoints_annotations.json' , img_dir = data_dir / 'images' ) records = parser . parse ( data_splitter = SingleSplitSplitter ())[ 0 ] record = records [ 1 ] show_record ( record , figsize = ( 10 , 10 ), class_map = class_map ) test_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 512 ), tfms . A . Normalize ()]) test_ds = Dataset ( records , test_tfms ) show_sample ( test_ds [ 0 ], figsize = ( 10 , 10 ), display_bbox = False )","title":"COCO Keypoints"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing IceVision \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request 2. Confirm PR was created: Ensure your PR is listed here 3. Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well. Resolving Conflicts In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-icevision","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install","title":"Step 1: Forking and Installing IceVision"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created","text":"Ensure your PR is listed here","title":"2. Confirm PR was created:"},{"location":"contributing/#3-updating-a-pr","text":"Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before>","title":"3.  Updating a PR:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#resolving-conflicts","text":"In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts.","title":"Resolving Conflicts"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parser - Simple This tutorial uses the Global Wheat Detection dataset, you can download it from Kaggle here . Instaling icevision ! pip install icevision [ all ] Imports As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd Understand the data format In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. source = Path ( \"/home/lgvaz/data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser . Create the Parser When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: parsers.FasterRCNN : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. parsers.FilepathMixin : Provides the requirements for parsing images filepaths. parsers.SizeMixin : Provides the requirements for parsing the image dimensions. The first step is to create a class that inherits from these smaller building blocks: class WheatParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): pass We now use a method generate_template that will print out all the necessary methods we have to implement. WheatParser . generate_template () def __iter__(self) -> Any: def imageid(self, o) -> Hashable: def image_width_height(self, o) -> Tuple[int, int]: return get_image_size(self.filepath(o)) def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: With this, we know what methods we have to implement and what each one should return (thanks to the type annotations)! Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ (a single DataFrame row here). Important Be sure to return the correct type on all overriden methods! class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): def __init__ ( self , df , source ): self . df = df self . source = source def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . image_id def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def image_width_height ( self , o ) -> Tuple [ int , int ]: return get_image_size ( self . filepath ( o )) def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Let's randomly split the data and parser with Parser.parse : parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse () Let's take a look at one record: show_record ( train_rs [ 0 ], display_label = False ) Conclusion And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that IceVision provides! Happy Learning! If you need any assistance, feel free to join our forum .","title":"Custom Parser"},{"location":"custom_parser/#custom-parser-simple","text":"This tutorial uses the Global Wheat Detection dataset, you can download it from Kaggle here .","title":"Custom Parser - Simple"},{"location":"custom_parser/#instaling-icevision","text":"! pip install icevision [ all ]","title":"Instaling icevision"},{"location":"custom_parser/#imports","text":"As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd","title":"Imports"},{"location":"custom_parser/#understand-the-data-format","text":"In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. source = Path ( \"/home/lgvaz/data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser .","title":"Understand the data format"},{"location":"custom_parser/#create-the-parser","text":"When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: parsers.FasterRCNN : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. parsers.FilepathMixin : Provides the requirements for parsing images filepaths. parsers.SizeMixin : Provides the requirements for parsing the image dimensions. The first step is to create a class that inherits from these smaller building blocks: class WheatParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): pass We now use a method generate_template that will print out all the necessary methods we have to implement. WheatParser . generate_template () def __iter__(self) -> Any: def imageid(self, o) -> Hashable: def image_width_height(self, o) -> Tuple[int, int]: return get_image_size(self.filepath(o)) def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: With this, we know what methods we have to implement and what each one should return (thanks to the type annotations)! Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ (a single DataFrame row here). Important Be sure to return the correct type on all overriden methods! class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): def __init__ ( self , df , source ): self . df = df self . source = source def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . image_id def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def image_width_height ( self , o ) -> Tuple [ int , int ]: return get_image_size ( self . filepath ( o )) def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Let's randomly split the data and parser with Parser.parse : parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse () Let's take a look at one record: show_record ( train_rs [ 0 ], display_label = False )","title":"Create the Parser"},{"location":"custom_parser/#conclusion","text":"And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that IceVision provides!","title":"Conclusion"},{"location":"custom_parser/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"data_splits/","text":"[source] DataSplitter icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source] RandomSplitter icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source] FixedSplitter icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source] SingleSplitSplitter icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"Data Splitters"},{"location":"data_splits/#datasplitter","text":"icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source]","title":"DataSplitter"},{"location":"data_splits/#randomsplitter","text":"icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source]","title":"RandomSplitter"},{"location":"data_splits/#fixedsplitter","text":"icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source]","title":"FixedSplitter"},{"location":"data_splits/#singlesplitsplitter","text":"icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"SingleSplitSplitter"},{"location":"dataset/","text":"[source] Dataset icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm icevision.tfms.transform.Transform : Transforms to be applied to each item. [source] from_images Dataset . from_images ( images , tfm = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"Dataset"},{"location":"dataset/#dataset","text":"icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm icevision.tfms.transform.Transform : Transforms to be applied to each item. [source]","title":"Dataset"},{"location":"dataset/#from_images","text":"Dataset . from_images ( images , tfm = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"from_images"},{"location":"dataset_voc_nb/","text":"How to train a voc compatible dataset. Note: This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating both data, and parsers files as opposed to the fridge dataset example. Installing IceVision ! pip install icevision [ all ] Clone the raccoom dataset repository ! git clone https : // github . com / datitran / raccoon_dataset Imports from icevision.all import * WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above Set images and annotations directories data_dir = Path ( 'raccoon_dataset' ) images_dir = data_dir / 'images' annotations_dir = data_dir / 'annotations' Define class_map class_map = ClassMap ([ 'raccoon' ]) Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Datasets Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) Fastai Learner metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Fastai Training Learning Rate Finder learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.10964782238006592, lr_steep=0.9120108485221863) Fine tune: 2 Phases Phase 1: Train the head for 5 epochs while freezing the body Phase 2: Train both the body and the head during 20 epochs learn . fine_tune ( 20 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.927721 1.351568 0.000867 00:08 1 1.621056 1.375566 0.002002 00:07 2 1.401676 1.202271 0.130699 00:07 3 1.176787 1.099883 0.257632 00:07 4 1.018470 1.031241 0.240425 00:07 epoch train_loss valid_loss COCOMetric time 0 0.549507 0.923337 0.276733 00:09 1 0.535427 0.802162 0.383750 00:09 2 0.518957 0.778002 0.402308 00:09 3 0.488329 0.722738 0.451429 00:09 4 0.469851 0.719689 0.364621 00:09 5 0.457274 0.583479 0.438622 00:09 6 0.436571 0.597234 0.424187 00:08 7 0.423843 0.641547 0.370738 00:09 8 0.407281 0.592756 0.430994 00:09 9 0.395995 0.550204 0.507850 00:09 10 0.386040 0.569015 0.408947 00:09 11 0.376276 0.556436 0.502973 00:10 12 0.368530 0.506910 0.485442 00:10 13 0.354401 0.557794 0.452313 00:09 14 0.342909 0.517615 0.488974 00:10 15 0.328028 0.511221 0.512096 00:09 16 0.320078 0.488857 0.510211 00:10 17 0.308560 0.489812 0.506092 00:09 18 0.303276 0.478933 0.524952 00:09 19 0.297155 0.470403 0.524153 00:10 Show results efficientdet . show_results ( model , valid_ds , class_map = class_map ) Note: You might train the model longer in order to increase its accuracy Happy Learning! If you need any assistance, feel free to join our forum .","title":"Training a VOC dataset"},{"location":"dataset_voc_nb/#how-to-train-a-voc-compatible-dataset","text":"Note: This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating both data, and parsers files as opposed to the fridge dataset example.","title":"How to train a voc compatible dataset."},{"location":"dataset_voc_nb/#installing-icevision","text":"! pip install icevision [ all ]","title":"Installing IceVision"},{"location":"dataset_voc_nb/#clone-the-raccoom-dataset-repository","text":"! git clone https : // github . com / datitran / raccoon_dataset","title":"Clone the raccoom dataset repository"},{"location":"dataset_voc_nb/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"dataset_voc_nb/#warning","text":"Make sure you have already cloned the raccoon dataset using the command shown here above","title":"WARNING:"},{"location":"dataset_voc_nb/#set-images-and-annotations-directories","text":"data_dir = Path ( 'raccoon_dataset' ) images_dir = data_dir / 'images' annotations_dir = data_dir / 'annotations'","title":"Set images and annotations directories"},{"location":"dataset_voc_nb/#define-class_map","text":"class_map = ClassMap ([ 'raccoon' ])","title":"Define class_map"},{"location":"dataset_voc_nb/#parser-use-icevision-predefined-voc-parser","text":"parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map )","title":"Parser: Use icevision predefined VOC parser"},{"location":"dataset_voc_nb/#train-and-validation-records","text":"train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"train and validation records"},{"location":"dataset_voc_nb/#datasets","text":"","title":"Datasets"},{"location":"dataset_voc_nb/#transforms","text":"presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"dataset_voc_nb/#train-and-validation-dataset-objects","text":"train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Train and Validation Dataset Objects"},{"location":"dataset_voc_nb/#dataloaders","text":"train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoaders"},{"location":"dataset_voc_nb/#model","text":"model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size )","title":"Model"},{"location":"dataset_voc_nb/#fastai-learner","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Fastai Learner"},{"location":"dataset_voc_nb/#fastai-training","text":"","title":"Fastai Training"},{"location":"dataset_voc_nb/#learning-rate-finder","text":"learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.10964782238006592, lr_steep=0.9120108485221863)","title":"Learning Rate Finder"},{"location":"dataset_voc_nb/#fine-tune-2-phases","text":"Phase 1: Train the head for 5 epochs while freezing the body Phase 2: Train both the body and the head during 20 epochs learn . fine_tune ( 20 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.927721 1.351568 0.000867 00:08 1 1.621056 1.375566 0.002002 00:07 2 1.401676 1.202271 0.130699 00:07 3 1.176787 1.099883 0.257632 00:07 4 1.018470 1.031241 0.240425 00:07 epoch train_loss valid_loss COCOMetric time 0 0.549507 0.923337 0.276733 00:09 1 0.535427 0.802162 0.383750 00:09 2 0.518957 0.778002 0.402308 00:09 3 0.488329 0.722738 0.451429 00:09 4 0.469851 0.719689 0.364621 00:09 5 0.457274 0.583479 0.438622 00:09 6 0.436571 0.597234 0.424187 00:08 7 0.423843 0.641547 0.370738 00:09 8 0.407281 0.592756 0.430994 00:09 9 0.395995 0.550204 0.507850 00:09 10 0.386040 0.569015 0.408947 00:09 11 0.376276 0.556436 0.502973 00:10 12 0.368530 0.506910 0.485442 00:10 13 0.354401 0.557794 0.452313 00:09 14 0.342909 0.517615 0.488974 00:10 15 0.328028 0.511221 0.512096 00:09 16 0.320078 0.488857 0.510211 00:10 17 0.308560 0.489812 0.506092 00:09 18 0.303276 0.478933 0.524952 00:09 19 0.297155 0.470403 0.524153 00:10","title":"Fine tune: 2 Phases"},{"location":"dataset_voc_nb/#show-results","text":"efficientdet . show_results ( model , valid_ds , class_map = class_map ) Note: You might train the model longer in order to increase its accuracy","title":"Show results"},{"location":"dataset_voc_nb/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"deployment/","text":"Deployment We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Overview"},{"location":"deployment/#deployment","text":"We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"efficientdet/","text":"[source] model icevision . models . efficientdet . model . model ( model_name , num_classes , img_size , pretrained = True ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments model_name str : Specifies the model to create. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. pretrained bool : If True, use a pretrained backbone (on COCO). Returns A PyTorch model. [source] train_dl icevision . models . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . efficientdet . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . efficientdet . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . efficientdet . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. # Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"efficientdet/#model","text":"icevision . models . efficientdet . model . model ( model_name , num_classes , img_size , pretrained = True ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments model_name str : Specifies the model to create. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. pretrained bool : If True, use a pretrained backbone (on COCO). Returns A PyTorch model. [source]","title":"model"},{"location":"efficientdet/#train_dl","text":"icevision . models . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"efficientdet/#valid_dl","text":"icevision . models . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"efficientdet/#infer_dl","text":"icevision . models . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"efficientdet/#build_train_batch","text":"icevision . models . efficientdet . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"efficientdet/#build_valid_batch","text":"icevision . models . efficientdet . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"efficientdet/#build_infer_batch","text":"icevision . models . efficientdet . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. # Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"efficientdet_fastai/","text":"[source] learner icevision . models . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"efficientdet_fastai/#learner","text":"icevision . models . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"efficientdet_lightning/","text":"[source] ModelAdapter icevision . models . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"efficientdet_lightning/#modeladapter","text":"icevision . models . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"efficientdet_pets/","text":"How to use EfficientDet Installing IceVision ! pip install icevision [ all ] icedata Imports from icevision.all import * Common part to all models Loading Data data_dir = icedata . pets . load () Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Datasets presize = 512 # EffecientDet requires the image size to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) EffecientDet Specific Part DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Fastai Training learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Inference DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) imgs = [ sample [ 'img' ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"EffecientDet"},{"location":"efficientdet_pets/#how-to-use-efficientdet","text":"","title":"How to use EfficientDet"},{"location":"efficientdet_pets/#installing-icevision","text":"! pip install icevision [ all ] icedata","title":"Installing IceVision"},{"location":"efficientdet_pets/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"efficientdet_pets/#common-part-to-all-models","text":"","title":"Common part to all models"},{"location":"efficientdet_pets/#loading-data","text":"data_dir = icedata . pets . load ()","title":"Loading Data"},{"location":"efficientdet_pets/#parser","text":"class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Parser"},{"location":"efficientdet_pets/#datasets","text":"presize = 512 # EffecientDet requires the image size to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet )","title":"Datasets"},{"location":"efficientdet_pets/#effecientdet-specific-part","text":"","title":"EffecientDet Specific Part"},{"location":"efficientdet_pets/#dataloaders","text":"train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoaders"},{"location":"efficientdet_pets/#model","text":"model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size )","title":"Model"},{"location":"efficientdet_pets/#fastai-learner","text":"metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Fastai Learner"},{"location":"efficientdet_pets/#fastai-training","text":"learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Training"},{"location":"efficientdet_pets/#inference","text":"","title":"Inference"},{"location":"efficientdet_pets/#dataloader","text":"infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 )","title":"DataLoader"},{"location":"efficientdet_pets/#predict","text":"samples , preds = efficientdet . predict_dl ( model , infer_dl ) imgs = [ sample [ 'img' ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Predict"},{"location":"efficientdet_pets/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"faster_rcnn/","text":"[source] model icevision . models . torchvision_models . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , pretrained = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained bool : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source] train_dl icevision . models . torchvision_models . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision_models . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision_models . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision_models . faster_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision_models . faster_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision_models . faster_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"faster_rcnn/#model","text":"icevision . models . torchvision_models . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , pretrained = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained bool : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source]","title":"model"},{"location":"faster_rcnn/#train_dl","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"faster_rcnn/#valid_dl","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"faster_rcnn/#infer_dl","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"faster_rcnn/#build_train_batch","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"faster_rcnn/#build_valid_batch","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"faster_rcnn/#build_infer_batch","text":"icevision . models . torchvision_models . faster_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"faster_rcnn_fastai/","text":"[source] learner icevision . models . torchvision_models . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"faster_rcnn_fastai/#learner","text":"icevision . models . torchvision_models . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"faster_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision_models . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"faster_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision_models . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"getting_started/","text":"Getting started with IceVision Why IceVision? IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly Introduction This tutorial walks you through the different steps of training and using a model. The IceVision Framework is an agnostic framework . To demonstrate this we will train and use our model with both the fastai , and pytorch-lightning libraries. If you are using Google Colab, the GPU runtime should be enabled, but if you experience problems when training your model, you may want to check this. Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU Install icevision and icedata ! pip install icevision [ all ] ! pip install icedata Import the package from icevision.all import * import icedata Datasets IceVision provides handy methods to load a dataset, parse annotations, and more. In the example below, we work with the PETS dataset to detect cats and dogs in images and identify their species. Loading the PETS dataset is one line code. data_dir = icedata . pets . load_data () data_dir Parser The Parser is one of the most important concepts in IceVision. It allows us to work with any annotation format. The basic job of the parser is to convert a custom format to something the library can understand. You might still need to create a custom parser for your own dataset. Fear not! Creating parsers is easy. After you've finished this tutorial, check this custom parser documentation to understand how to. IceVision already provides a parser for the Pets Dataset class_map = icedata . pets . class_map () class_map parser = icedata . pets . parser ( data_dir , class_map ) Parse the data Next we parse() the dataset using the data splitter. This returns returns 2 lists of records: one for training and another for validation. Behind the scenes we shuffle the data and proceed with a 80% train 20% valid split. train_records , valid_records = parser . parse () What's a record? A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotation has, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.) Visualize the training data We can show one of the records (image + box + label). This helps to understand what is in the dataset and check that the boxes and labels make sense. show_record ( train_records [ 1 ]) We can also display the label instead of its identifier by providing the class_map . show_record ( train_records [ 1 ], class_map = class_map ) Of course, we often want to see several images with their corresponding boxes and labels. records = train_records [: 6 ] show_records ( records , ncols = 3 , class_map = class_map ) Transforms Data transformations are an essential part of the training pipeline. There are many transformation libraries available including: albumentations , solt , and torchvision . IceVision supports the widely used albumentations library out-of-the-box. It is possible to integrate other transform libraries. You just need to inherit and override all abstract methods of the Transform class. We plan to add more to future versions in response to community feedback. It is typical to use different transformations for the training and validation datasets. The valid_tfms apply to the validation set. These are minimal - just resizing the image and normalising it. The train_tfms typically do data augmentations such as zoom, crop, lighting adjustments, horizontal flips, and so on. These help to reduce the required training set size, reduce overfitting, and produce a more robust model. Icevision makes this easy - all of the bounding boxes are adjusted if needed. For example, zooming in will make the bounding boxes larger. Crops will not cut any bounding boxes. The presize parameter helps to improve the resulting image quality. See the Fast AI Book for more details. The A.Normalize function applies a set of default normalizations that have been refined over the years on the Imagenet dataset. presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) Dataset The Dataset class combines the records and transforms. To create a Dataset , we just need need to pass the parsed records from the previous step along with the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What does the Dataset class do? Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. Applies the pipeline of transforms to the record prepared in the previous step Lazy transforms Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. Important Because we normalized our images with imagenet_stats , when displaying transformed images, we need to denormalize them. The show_sample function receives an optional argument called denormalize_fn that we can be passed: In our case, we pass denormalize_imagenet . Displaying the same image with different transforms samples = [ train_ds [ 3 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , class_map = class_map ) Model In this tutorial, we are learning to predict bounding boxes and classes, but not performing image segmentation. We will use the FasterRCNN model. To create the model, we need to specify how many classes our dataset has. This is the length of the class_map . Note that the class_map includes a value for \"background\" with index 0, which is added behind the scenes by default. model = faster_rcnn . model ( num_classes = len ( class_map )) DataLoader Each model has its own dataloader (a pytorch DataLoader ) that could be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Training IceVision is an agnostic framework meaning it can be plugged to multiple DL frameworks such as fastai , and pytorch-lightning . You could also plug it into a new DL frameworks using your own custom code. Metrics Metrics are essential for tracking the model progress as it's training. Here we are going to be using the well established COCOMetric , which reports on the mean average precision of the predictions. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training with fastai Creating a Learner object Creating a fastai compatible Learner using the fastai interface. learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Training the RCNN model using fastai fine_tune() method The fastai fine_tune method is useful when you have a pre-trained model, which we are using. It does an initial epoch where it freezes everything except its final layers. It then carries on for the indicated number of epochs using a differential learning rate to train the whole model. It adjusts the learning rate both across the layers of the model as well as across the epochs. This can give excellent results with reduced training time. In September 2020, if everything is working, the model might require around 3 minutes per epoch on a free Google Colab server. learn . fine_tune ( 10 , 1e-4 ) Training with Pytorch-Lightning Creating a Pytorch-Lightning (PL) model class It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) **Note:** If you are familiar to working with lightning, you will note that we've been able to skip some of the boilerplate. This is because the IceVision `RCNNLightningAdapter` takes care of it behind the scene. For example, it defines `training_step` and `validation_step`. The adaptor also supports working with `Metric`s. If you need custom functionality, you can override or re-implement those methods. light_model = LightModel ( model , metrics = metrics ) Training the RCNN model using PL Trainer.fit() method trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Visualize results To quickly visualize the results of the model on a specific dataset use show_results : faster_rcnn . show_results ( model , valid_ds , class_map = class_map ) Inference Load a model Training the model with fastai using fine_tune twice and I got led the the following results: train_loss: 0.06772 valid_loss: 0.074435 Using our Trained Weights If you don't want to train the model, you can use our trained weights that we publicly available: You can download them with torch.hub : weights_url = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cpu\" )) Note Typically inference is done on the cpu, this is why we specify the paramater map_location to cpu when loading the state dict. Let's recreate the model and load the downloaded weights: model = faster_rcnn . model ( num_classes = len ( class_map )) model . load_state_dict ( state_dict ) model . cuda () The first step for prediction is to have some images, let's grab some random ones from the validation dataset: 11.3- Predict all images at once If you don't have too many images, you can get predictions with a single forward pass. In case your images don't fit in memory simultaneously, you should predict in batches, the next section shows how to do that. For demonstration purposes, let's take download a single image from the internet and see how our model performs on it. IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img ) <AxesSubplot:> Try other images! Change IMAGE_URL to point to another image you found on the internet. Just be sure to take one of the breeds from class_map , or else the model might get confused. Whenever you have images in memory (numpy arrays) you can use Dataset.from_images . We're going to use the same transforms we used on the validation dataset. infer_ds = Dataset . from_images ([ img ], valid_tfms ) For any model, the prediction steps are always the same, first call build_infer_batch and then predict . For faster_rcnn we have detection_threshold , which specifies how confident the model should be to output a bounding box. batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) For displaying the predictions, we first need to grab our image from samples . We do this instead of using the original images because transforms may have been applied to the image (in fact, in this case, a resize was used). imgs = [ sample [ \"img\" ] for sample in samples ] Now we just need to call show_preds , to show the image with its corresponding predictions (boxes + labels). show_preds ( imgs = imgs , preds = preds , class_map = class_map , show = True ) 11.4- Predicting a batch of images Instead of predicting a whole list of images at one, we can process a small batch at the time: This option is more memory efficient: We use infer_dataloader Had we have a test dataset, we would have maken our predicition using the batch technique mentionned here above. As an illustrative example, we will predict all images belonging to the validation dataset using the following approach: infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Same as before, we grab our images from samples . imgs = [ sample [ \"img\" ] for sample in samples ] Let's show the first 6 predictions: show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], ncols = 3 , class_map = class_map , show = True , ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Getting Started"},{"location":"getting_started/#getting-started-with-icevision","text":"","title":"Getting started with IceVision"},{"location":"getting_started/#why-icevision","text":"IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly","title":"Why IceVision?"},{"location":"getting_started/#introduction","text":"This tutorial walks you through the different steps of training and using a model. The IceVision Framework is an agnostic framework . To demonstrate this we will train and use our model with both the fastai , and pytorch-lightning libraries. If you are using Google Colab, the GPU runtime should be enabled, but if you experience problems when training your model, you may want to check this. Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU","title":"Introduction"},{"location":"getting_started/#install-icevision-and-icedata","text":"! pip install icevision [ all ] ! pip install icedata","title":"Install icevision and icedata"},{"location":"getting_started/#import-the-package","text":"from icevision.all import * import icedata","title":"Import the package"},{"location":"getting_started/#datasets","text":"IceVision provides handy methods to load a dataset, parse annotations, and more. In the example below, we work with the PETS dataset to detect cats and dogs in images and identify their species. Loading the PETS dataset is one line code. data_dir = icedata . pets . load_data () data_dir","title":"Datasets"},{"location":"getting_started/#parser","text":"The Parser is one of the most important concepts in IceVision. It allows us to work with any annotation format. The basic job of the parser is to convert a custom format to something the library can understand. You might still need to create a custom parser for your own dataset. Fear not! Creating parsers is easy. After you've finished this tutorial, check this custom parser documentation to understand how to. IceVision already provides a parser for the Pets Dataset class_map = icedata . pets . class_map () class_map parser = icedata . pets . parser ( data_dir , class_map )","title":"Parser"},{"location":"getting_started/#parse-the-data","text":"Next we parse() the dataset using the data splitter. This returns returns 2 lists of records: one for training and another for validation. Behind the scenes we shuffle the data and proceed with a 80% train 20% valid split. train_records , valid_records = parser . parse () What's a record? A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotation has, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.)","title":"Parse the data"},{"location":"getting_started/#visualize-the-training-data","text":"We can show one of the records (image + box + label). This helps to understand what is in the dataset and check that the boxes and labels make sense. show_record ( train_records [ 1 ]) We can also display the label instead of its identifier by providing the class_map . show_record ( train_records [ 1 ], class_map = class_map ) Of course, we often want to see several images with their corresponding boxes and labels. records = train_records [: 6 ] show_records ( records , ncols = 3 , class_map = class_map )","title":"Visualize the training data"},{"location":"getting_started/#transforms","text":"Data transformations are an essential part of the training pipeline. There are many transformation libraries available including: albumentations , solt , and torchvision . IceVision supports the widely used albumentations library out-of-the-box. It is possible to integrate other transform libraries. You just need to inherit and override all abstract methods of the Transform class. We plan to add more to future versions in response to community feedback. It is typical to use different transformations for the training and validation datasets. The valid_tfms apply to the validation set. These are minimal - just resizing the image and normalising it. The train_tfms typically do data augmentations such as zoom, crop, lighting adjustments, horizontal flips, and so on. These help to reduce the required training set size, reduce overfitting, and produce a more robust model. Icevision makes this easy - all of the bounding boxes are adjusted if needed. For example, zooming in will make the bounding boxes larger. Crops will not cut any bounding boxes. The presize parameter helps to improve the resulting image quality. See the Fast AI Book for more details. The A.Normalize function applies a set of default normalizations that have been refined over the years on the Imagenet dataset. presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"getting_started/#dataset","text":"The Dataset class combines the records and transforms. To create a Dataset , we just need need to pass the parsed records from the previous step along with the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What does the Dataset class do? Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. Applies the pipeline of transforms to the record prepared in the previous step Lazy transforms Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. Important Because we normalized our images with imagenet_stats , when displaying transformed images, we need to denormalize them. The show_sample function receives an optional argument called denormalize_fn that we can be passed: In our case, we pass denormalize_imagenet .","title":"Dataset"},{"location":"getting_started/#displaying-the-same-image-with-different-transforms","text":"samples = [ train_ds [ 3 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"getting_started/#model","text":"In this tutorial, we are learning to predict bounding boxes and classes, but not performing image segmentation. We will use the FasterRCNN model. To create the model, we need to specify how many classes our dataset has. This is the length of the class_map . Note that the class_map includes a value for \"background\" with index 0, which is added behind the scenes by default. model = faster_rcnn . model ( num_classes = len ( class_map ))","title":"Model"},{"location":"getting_started/#dataloader","text":"Each model has its own dataloader (a pytorch DataLoader ) that could be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoader"},{"location":"getting_started/#training","text":"IceVision is an agnostic framework meaning it can be plugged to multiple DL frameworks such as fastai , and pytorch-lightning . You could also plug it into a new DL frameworks using your own custom code.","title":"Training"},{"location":"getting_started/#metrics","text":"Metrics are essential for tracking the model progress as it's training. Here we are going to be using the well established COCOMetric , which reports on the mean average precision of the predictions. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"getting_started/#training-with-fastai","text":"","title":"Training with fastai"},{"location":"getting_started/#creating-a-learner-object","text":"Creating a fastai compatible Learner using the fastai interface. learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Creating a Learner object"},{"location":"getting_started/#training-the-rcnn-model-using-fastai-fine_tune-method","text":"The fastai fine_tune method is useful when you have a pre-trained model, which we are using. It does an initial epoch where it freezes everything except its final layers. It then carries on for the indicated number of epochs using a differential learning rate to train the whole model. It adjusts the learning rate both across the layers of the model as well as across the epochs. This can give excellent results with reduced training time. In September 2020, if everything is working, the model might require around 3 minutes per epoch on a free Google Colab server. learn . fine_tune ( 10 , 1e-4 )","title":"Training the RCNN model using fastai fine_tune() method"},{"location":"getting_started/#training-with-pytorch-lightning","text":"","title":"Training with Pytorch-Lightning"},{"location":"getting_started/#creating-a-pytorch-lightning-pl-model-class","text":"It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) **Note:** If you are familiar to working with lightning, you will note that we've been able to skip some of the boilerplate. This is because the IceVision `RCNNLightningAdapter` takes care of it behind the scene. For example, it defines `training_step` and `validation_step`. The adaptor also supports working with `Metric`s. If you need custom functionality, you can override or re-implement those methods. light_model = LightModel ( model , metrics = metrics )","title":"Creating a Pytorch-Lightning (PL) model class"},{"location":"getting_started/#training-the-rcnn-model-using-pl-trainerfit-method","text":"trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training the RCNN model using PL Trainer.fit() method"},{"location":"getting_started/#visualize-results","text":"To quickly visualize the results of the model on a specific dataset use show_results : faster_rcnn . show_results ( model , valid_ds , class_map = class_map )","title":"Visualize results"},{"location":"getting_started/#inference","text":"","title":"Inference"},{"location":"getting_started/#load-a-model","text":"Training the model with fastai using fine_tune twice and I got led the the following results: train_loss: 0.06772 valid_loss: 0.074435","title":"Load a model"},{"location":"getting_started/#using-our-trained-weights","text":"If you don't want to train the model, you can use our trained weights that we publicly available: You can download them with torch.hub : weights_url = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cpu\" )) Note Typically inference is done on the cpu, this is why we specify the paramater map_location to cpu when loading the state dict. Let's recreate the model and load the downloaded weights: model = faster_rcnn . model ( num_classes = len ( class_map )) model . load_state_dict ( state_dict ) model . cuda () The first step for prediction is to have some images, let's grab some random ones from the validation dataset:","title":"Using our Trained Weights"},{"location":"getting_started/#113-predict-all-images-at-once","text":"If you don't have too many images, you can get predictions with a single forward pass. In case your images don't fit in memory simultaneously, you should predict in batches, the next section shows how to do that. For demonstration purposes, let's take download a single image from the internet and see how our model performs on it. IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img ) <AxesSubplot:> Try other images! Change IMAGE_URL to point to another image you found on the internet. Just be sure to take one of the breeds from class_map , or else the model might get confused. Whenever you have images in memory (numpy arrays) you can use Dataset.from_images . We're going to use the same transforms we used on the validation dataset. infer_ds = Dataset . from_images ([ img ], valid_tfms ) For any model, the prediction steps are always the same, first call build_infer_batch and then predict . For faster_rcnn we have detection_threshold , which specifies how confident the model should be to output a bounding box. batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) For displaying the predictions, we first need to grab our image from samples . We do this instead of using the original images because transforms may have been applied to the image (in fact, in this case, a resize was used). imgs = [ sample [ \"img\" ] for sample in samples ] Now we just need to call show_preds , to show the image with its corresponding predictions (boxes + labels). show_preds ( imgs = imgs , preds = preds , class_map = class_map , show = True )","title":"11.3- Predict all images at once"},{"location":"getting_started/#114-predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process a small batch at the time: This option is more memory efficient: We use infer_dataloader Had we have a test dataset, we would have maken our predicition using the batch technique mentionned here above. As an illustrative example, we will predict all images belonging to the validation dataset using the following approach: infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Same as before, we grab our images from samples . imgs = [ sample [ \"img\" ] for sample in samples ] Let's show the first 6 predictions: show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], ncols = 3 , class_map = class_map , show = True , )","title":"11.4- Predicting a batch of images"},{"location":"getting_started/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"how-to/","text":"Where can I get some help? If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum . How to install icevision? To install the IceVision package as well as all its dependencies, choose one of the 2 options: Installing the IceVision lastest version pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade Install the IceVision lastest version from Pypi repository: pip install icevision [ all ] For more options, and more in-depth explanation on how to install IceVision, please check out our Installation Guide How to create an EffecientDet Model? tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Model as well as the EffecientDet Backbone documents. How to create a Faster RCNN Model? fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents/ How to create a Mask RCNN Model? - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents. How to use EffecientDet Backbones? EffecientDet backbones are passed as string argument to the effecientdet model function: model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Backbone document. How to use Faster RCNN Backbones? Faster RCNN backbones are passed a model object argument to the Faster RCNN model function: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document. How to use Mask RCNN Backbones? Mask RCNN backbones are passed a model object argument to the Mask RCNN model function: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document. How to predict (infer) a single image? This is a quick example using the PETS dataset: # Imports from icevision.all import * # Maps from IDs to class names. `print(class_map)` for all available classes class_map = datasets . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained on `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , ) How to save trained weights in Google Colab? In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by IceVision Check out the Quick Start Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' ) How to load pretrained weights? In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) How to contribute? We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How-To"},{"location":"how-to/#where-can-i-get-some-help","text":"If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum .","title":"Where can I get some help?"},{"location":"how-to/#how-to-install-icevision","text":"To install the IceVision package as well as all its dependencies, choose one of the 2 options: Installing the IceVision lastest version pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade Install the IceVision lastest version from Pypi repository: pip install icevision [ all ] For more options, and more in-depth explanation on how to install IceVision, please check out our Installation Guide","title":"How to install icevision?"},{"location":"how-to/#how-to-create-an-effecientdet-model","text":"tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Model as well as the EffecientDet Backbone documents.","title":"How to create an EffecientDet Model?"},{"location":"how-to/#how-to-create-a-faster-rcnn-model","text":"fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents/","title":"How to create a Faster RCNN Model?"},{"location":"how-to/#how-to-create-a-mask-rcnn-model","text":"- Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents.","title":"How to create a Mask RCNN Model?"},{"location":"how-to/#how-to-use-effecientdet-backbones","text":"EffecientDet backbones are passed as string argument to the effecientdet model function: model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Backbone document.","title":"How to use EffecientDet Backbones?"},{"location":"how-to/#how-to-use-faster-rcnn-backbones","text":"Faster RCNN backbones are passed a model object argument to the Faster RCNN model function: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document.","title":"How to use Faster RCNN Backbones?"},{"location":"how-to/#how-to-use-mask-rcnn-backbones","text":"Mask RCNN backbones are passed a model object argument to the Mask RCNN model function: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document.","title":"How to use Mask RCNN Backbones?"},{"location":"how-to/#how-to-predict-infer-a-single-image","text":"This is a quick example using the PETS dataset: # Imports from icevision.all import * # Maps from IDs to class names. `print(class_map)` for all available classes class_map = datasets . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained on `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"How to predict (infer) a single image?"},{"location":"how-to/#how-to-save-trained-weights-in-google-colab","text":"In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by IceVision Check out the Quick Start Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' )","title":"How to save trained weights in Google Colab?"},{"location":"how-to/#how-to-load-pretrained-weights","text":"In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict )","title":"How to load pretrained weights?"},{"location":"how-to/#how-to-contribute","text":"We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How to contribute?"},{"location":"inference/","text":"Use a trained model for inference Install IceVision We usually install IceVision with [all] , but you can also use [inference] to only install the dependencies necessary for inference. ! pip install icevision [ all ] icedata Imports As always, import everything from IceVision: from icevision.all import * Loading the model We're going to use the model trained on the getting started tutorial. Saving/Loading a model Save your model with torch.save(model.state_dict(), PATH) . Take a look at this tutorial for more info on how to save/load models. The first thing we need is the ClassMap used during training. The model we're going to use was trained on the Pets dataset, so we can easily grab that: class_map = icedata . pets . class_map () Recreate the model used in training: model = faster_rcnn . model ( num_classes = len ( class_map )) And now load the model weights (commonly refered as state_dict in Pytorch). In our case, the weights are stored in the cloud, Pytorch is amazing and provides us with a function to directly load model weights from an URL. Where to host your model You can save your model directly on github via a \"release\" Simply go to https://github.com/<ACCOUNT>/<REPO>/releases/new and upload the model as a new release. This will generate a direct link for downloading the model. WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" )) If your weights are stored locally, you simply need to do: state_dict = torch.load(<PATH>) Model device Notice that we're asking to load the weights on CPU with map_location . It's common to do inference in CPU, but you can easily change to GPU if you want. Now, let's actually load the weights to the model: model . load_state_dict ( state_dict ) <All keys matched successfully> Transforms Generally speaking, we normally use on inference the same transforms we used in the validation set. A transform like normalize is always required, but for some models, a transform like resize is optional. For instance, the model used in this tutorial accepts any image resolution, try playing around with different transforms and observe how they change the model results. Let's use the same transforms used in the validation set: infer_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 384 ), tfms . A . Normalize ()]) Data Obviously we need some images to perform inference, how you get this images heavily depends on your use case. For this tutorial, let's grab an image from an URL: import PIL , requests def image_from_url ( url ): res = requests . get ( url , stream = True ) img = PIL . Image . open ( res . raw ) return np . array ( img ) image_url = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" img = image_from_url ( image_url ) show_img ( img ); Try it out Try experimenting with new images, be sure to take one of the breeds from class_map (or not, if you are curious to see what happens). Whenever we have images in memory (numpy arrays) we can use Dataset.from_images to easily create a Dataset from it: infer_ds = Dataset . from_images ([ img ], infer_tfms ) Predict - All at once First build a batch with the entire dataset, and then simply call predict : batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) Predict - In batches If the memory is not enough to predict everything at once, break it down into smaller batches with infer_dataloader : infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Visualize This step will probably not be the same for your use case, but for quickly visualizing the predictions we can use show_preds : # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( samples = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Inference"},{"location":"inference/#use-a-trained-model-for-inference","text":"","title":"Use a trained model for inference"},{"location":"inference/#install-icevision","text":"We usually install IceVision with [all] , but you can also use [inference] to only install the dependencies necessary for inference. ! pip install icevision [ all ] icedata","title":"Install IceVision"},{"location":"inference/#imports","text":"As always, import everything from IceVision: from icevision.all import *","title":"Imports"},{"location":"inference/#loading-the-model","text":"We're going to use the model trained on the getting started tutorial. Saving/Loading a model Save your model with torch.save(model.state_dict(), PATH) . Take a look at this tutorial for more info on how to save/load models. The first thing we need is the ClassMap used during training. The model we're going to use was trained on the Pets dataset, so we can easily grab that: class_map = icedata . pets . class_map () Recreate the model used in training: model = faster_rcnn . model ( num_classes = len ( class_map )) And now load the model weights (commonly refered as state_dict in Pytorch). In our case, the weights are stored in the cloud, Pytorch is amazing and provides us with a function to directly load model weights from an URL. Where to host your model You can save your model directly on github via a \"release\" Simply go to https://github.com/<ACCOUNT>/<REPO>/releases/new and upload the model as a new release. This will generate a direct link for downloading the model. WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" )) If your weights are stored locally, you simply need to do: state_dict = torch.load(<PATH>) Model device Notice that we're asking to load the weights on CPU with map_location . It's common to do inference in CPU, but you can easily change to GPU if you want. Now, let's actually load the weights to the model: model . load_state_dict ( state_dict ) <All keys matched successfully>","title":"Loading the model"},{"location":"inference/#transforms","text":"Generally speaking, we normally use on inference the same transforms we used in the validation set. A transform like normalize is always required, but for some models, a transform like resize is optional. For instance, the model used in this tutorial accepts any image resolution, try playing around with different transforms and observe how they change the model results. Let's use the same transforms used in the validation set: infer_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 384 ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"inference/#data","text":"Obviously we need some images to perform inference, how you get this images heavily depends on your use case. For this tutorial, let's grab an image from an URL: import PIL , requests def image_from_url ( url ): res = requests . get ( url , stream = True ) img = PIL . Image . open ( res . raw ) return np . array ( img ) image_url = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" img = image_from_url ( image_url ) show_img ( img ); Try it out Try experimenting with new images, be sure to take one of the breeds from class_map (or not, if you are curious to see what happens). Whenever we have images in memory (numpy arrays) we can use Dataset.from_images to easily create a Dataset from it: infer_ds = Dataset . from_images ([ img ], infer_tfms )","title":"Data"},{"location":"inference/#predict-all-at-once","text":"First build a batch with the entire dataset, and then simply call predict : batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch )","title":"Predict - All at once"},{"location":"inference/#predict-in-batches","text":"If the memory is not enough to predict everything at once, break it down into smaller batches with infer_dataloader : infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl )","title":"Predict - In batches"},{"location":"inference/#visualize","text":"This step will probably not be the same for your use case, but for quickly visualizing the predictions we can use show_preds : # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( samples = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"Visualize"},{"location":"inference/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"install/","text":"Important We currently only support Linux/MacOS installations A- Installation using pip Option 1: Installing from pypi repository [Stable Version] To install icevision package together with all dependencies: $ pip install icevision [ all ] Option 2: Installing an editable package locally [For Developers] Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e \".[all,dev]\" $ pre-commit install Option 3: Installing a non-editable package from GitHub: To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade B- Installation using conda Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called ice $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ] C- Fixing the Error: Failed building wheel for pycocotools If you encounter the Failed building wheel for pycocotools error (see screenshoot here below), you can easily fix it by installing gcc from your linux terminal as shown in the following steps: $ sudo apt update $ sudo apt install gcc Note You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient installation option for you.","title":"Installation"},{"location":"install/#a-installation-using-pip","text":"","title":"A- Installation using pip"},{"location":"install/#option-1-installing-from-pypi-repository-stable-version","text":"To install icevision package together with all dependencies: $ pip install icevision [ all ]","title":"Option 1: Installing from pypi repository [Stable Version]"},{"location":"install/#option-2-installing-an-editable-package-locally-for-developers","text":"Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e \".[all,dev]\" $ pre-commit install","title":"Option 2: Installing an editable package locally [For Developers]"},{"location":"install/#option-3-installing-a-non-editable-package-from-github","text":"To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade","title":"Option 3: Installing a non-editable package from GitHub:"},{"location":"install/#b-installation-using-conda","text":"Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called ice $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ]","title":"B- Installation using conda"},{"location":"install/#c-fixing-the-error-failed-building-wheel-for-pycocotools","text":"If you encounter the Failed building wheel for pycocotools error (see screenshoot here below), you can easily fix it by installing gcc from your linux terminal as shown in the following steps: $ sudo apt update $ sudo apt install gcc Note You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient installation option for you.","title":"C- Fixing the Error: Failed building wheel for pycocotools"},{"location":"mask_rcnn/","text":"[source] model icevision . models . torchvision_models . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , pretrained = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained bool : Argument passed to maskrcnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source] train_dl icevision . models . torchvision_models . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision_models . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision_models . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision_models . mask_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision_models . mask_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision_models . mask_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"mask_rcnn/#model","text":"icevision . models . torchvision_models . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , pretrained = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained bool : Argument passed to maskrcnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source]","title":"model"},{"location":"mask_rcnn/#train_dl","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"mask_rcnn/#valid_dl","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"mask_rcnn/#infer_dl","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"mask_rcnn/#build_train_batch","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"mask_rcnn/#build_valid_batch","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"mask_rcnn/#build_infer_batch","text":"icevision . models . torchvision_models . mask_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"mask_rcnn_fastai/","text":"[source] learner icevision . models . torchvision_models . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"mask_rcnn_fastai/#learner","text":"icevision . models . torchvision_models . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"mask_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision_models . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"mask_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision_models . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"mask_rcnn_pennfudan/","text":"How to use Mask RCNN Installing IceVision We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. ! pip install icevision [ all ] icedata Imports from icevision.all import * Data We'll be using the Penn-Fudan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) Metrics Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)] Model Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map )) Training - fastai We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss time 0 1.609195 0.804607 00:19 1 1.115979 0.536064 00:15 epoch train_loss valid_loss time 0 0.777911 0.435238 00:22 1 0.618750 0.363259 00:19 2 0.555032 0.341701 00:18 3 0.507919 0.335720 00:19 4 0.486197 0.330338 00:19 5 0.450105 0.278701 00:19 6 0.426801 0.280237 00:18 7 0.417322 0.296473 00:16 8 0.411688 0.289382 00:17 9 0.401172 0.283575 00:17 Visualize predictions Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Mask RCNN"},{"location":"mask_rcnn_pennfudan/#how-to-use-mask-rcnn","text":"","title":"How to use Mask RCNN"},{"location":"mask_rcnn_pennfudan/#installing-icevision","text":"We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. ! pip install icevision [ all ] icedata","title":"Installing IceVision"},{"location":"mask_rcnn_pennfudan/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"mask_rcnn_pennfudan/#data","text":"We'll be using the Penn-Fudan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 )","title":"Data"},{"location":"mask_rcnn_pennfudan/#metrics","text":"Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)]","title":"Metrics"},{"location":"mask_rcnn_pennfudan/#model","text":"Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map ))","title":"Model"},{"location":"mask_rcnn_pennfudan/#training-fastai","text":"We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss time 0 1.609195 0.804607 00:19 1 1.115979 0.536064 00:15 epoch train_loss valid_loss time 0 0.777911 0.435238 00:22 1 0.618750 0.363259 00:19 2 0.555032 0.341701 00:18 3 0.507919 0.335720 00:19 4 0.486197 0.330338 00:19 5 0.450105 0.278701 00:19 6 0.426801 0.280237 00:18 7 0.417322 0.296473 00:16 8 0.411688 0.289382 00:17 9 0.401172 0.283575 00:17","title":"Training - fastai"},{"location":"mask_rcnn_pennfudan/#visualize-predictions","text":"Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map )","title":"Visualize predictions"},{"location":"mask_rcnn_pennfudan/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"model_comparison/","text":"Models Source IceVision offers both EffecientDet and Faster RCNN models. IceVision uses a unified API that makes it easy for the users to swap one model by another as it is shown in the following example. Click one or the other tab to compare both implementations and discover the strong similarities between the two implementations: EffecientDet # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Faster RCNN # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Faster RCNN Specific Part # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = faster_rcnn . model ( num_classes = len ( class_map )) # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Models Comparison"},{"location":"model_comparison/#models","text":"Source IceVision offers both EffecientDet and Faster RCNN models. IceVision uses a unified API that makes it easy for the users to swap one model by another as it is shown in the following example. Click one or the other tab to compare both implementations and discover the strong similarities between the two implementations: EffecientDet # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Faster RCNN # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Faster RCNN Specific Part # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = faster_rcnn . model ( num_classes = len ( class_map )) # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Models"},{"location":"model_efficientdet/","text":"EffecientDet Model Source EffecientDet is one of the effecient and fastest object detection model that also uses more constrained resources in comparison to other models (Fig. 1). It was introduced in the following paper: EfficientDet: Scalable and Efficient Object Detection Usage To train an EffecientDet model, you need to call the following highlighted functions shown here below. Common part to both Fastai and Pytorch-Lightning Training Loop DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates an EffecientDet model implemented by Ross Wightman . The model accepts a variety of backbones. In following example, the tf_efficientdet_lite0 is used. We can also choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) How to use a different backbone: \"efficientdet_d0\" example # Model model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) Fastai Example Once the DataLoaders and the EffecientDet model are created, we create the fastai Learner. The latter uses the DataLoaders and the EffecientDet model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the EffecientDet model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Pytorch-Lightning Example The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same EffecientDet model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the EffecientDet model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Train using pytorch-lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) How to train the PETS Dataset using EffecientDet Source Code Background: Paper Abstract Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detectors. References Paper EfficientDet: Scalable and Efficient Object Detection Implementation We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. Currently this includes all models implemented by the EficientNet and MobileNetv3 classes (which also includes MNasNet, MobileNetV2, MixNet and more)","title":"EffecientDet"},{"location":"model_efficientdet/#effecientdet-model","text":"Source EffecientDet is one of the effecient and fastest object detection model that also uses more constrained resources in comparison to other models (Fig. 1). It was introduced in the following paper: EfficientDet: Scalable and Efficient Object Detection","title":"EffecientDet Model"},{"location":"model_efficientdet/#usage","text":"To train an EffecientDet model, you need to call the following highlighted functions shown here below.","title":"Usage"},{"location":"model_efficientdet/#common-part-to-both-fastai-and-pytorch-lightning-training-loop","text":"DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates an EffecientDet model implemented by Ross Wightman . The model accepts a variety of backbones. In following example, the tf_efficientdet_lite0 is used. We can also choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) How to use a different backbone: \"efficientdet_d0\" example # Model model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size )","title":"Common part to both Fastai and Pytorch-Lightning Training Loop"},{"location":"model_efficientdet/#fastai-example","text":"Once the DataLoaders and the EffecientDet model are created, we create the fastai Learner. The latter uses the DataLoaders and the EffecientDet model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the EffecientDet model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Example"},{"location":"model_efficientdet/#pytorch-lightning-example","text":"The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same EffecientDet model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the EffecientDet model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Train using pytorch-lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Pytorch-Lightning Example"},{"location":"model_efficientdet/#how-to-train-the-pets-dataset-using-effecientdet","text":"Source Code","title":"How to train the PETS Dataset using EffecientDet"},{"location":"model_efficientdet/#background-paper-abstract","text":"Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detectors.","title":"Background: Paper Abstract"},{"location":"model_efficientdet/#references","text":"","title":"References"},{"location":"model_efficientdet/#paper","text":"EfficientDet: Scalable and Efficient Object Detection","title":"Paper"},{"location":"model_efficientdet/#implementation","text":"We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. Currently this includes all models implemented by the EficientNet and MobileNetv3 classes (which also includes MNasNet, MobileNetV2, MixNet and more)","title":"Implementation"},{"location":"model_faster_rcnn/","text":"Faster RCNN Model Source Faster RCNN is one of the most popular object detection model. It was introduced in the following paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Usage To train a Faster RCNN model, you need to call the following highlighted functions shown here below. Common part to both Fastai and Pytorch-Lightning Training Loop DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates a Faster RCNN model implemented in torchvision FasterRCNN . The model accepts a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the following backbones : resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 # Model model = faster_rcnn . model ( num_classes = len ( class_map )) How to use a different backbone: \"resnet18\" example # Backbone backbone = faster_rcnn . backbones . resnet_fpn . resnet18 ( pretrained = True ) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) Fastai Example Once the DataLoaders and the Faster RCNN model are created, we create the fastai Learner. The latter uses the DataLoaders and the Faster RCNN model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the Faster RCNN model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Pytorch-Lightning Example The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same Faster RCNN model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the Faster RCNN model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) How to train the PETS Dataset using Faster RCNN Source Code Paper Introduction Faster R-CNN is built upon the knowledge of Fast RCNN which indeed built upon the ideas of RCNN and SPP-Net. In their paper, the authors introduced a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. References Paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Blog Posts An overview of deep-learning based object-detection algorithms Guide to build Faster RCNN in PyTorch Torchvision Implementation Torchvision Object Detection Finetuning Tutorial Torchvision Faster RCNN Implementation","title":"Faster RCNN"},{"location":"model_faster_rcnn/#faster-rcnn-model","text":"Source Faster RCNN is one of the most popular object detection model. It was introduced in the following paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks","title":"Faster RCNN Model"},{"location":"model_faster_rcnn/#usage","text":"To train a Faster RCNN model, you need to call the following highlighted functions shown here below.","title":"Usage"},{"location":"model_faster_rcnn/#common-part-to-both-fastai-and-pytorch-lightning-training-loop","text":"DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates a Faster RCNN model implemented in torchvision FasterRCNN . The model accepts a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the following backbones : resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 # Model model = faster_rcnn . model ( num_classes = len ( class_map )) How to use a different backbone: \"resnet18\" example # Backbone backbone = faster_rcnn . backbones . resnet_fpn . resnet18 ( pretrained = True ) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ))","title":"Common part to both Fastai and Pytorch-Lightning Training Loop"},{"location":"model_faster_rcnn/#fastai-example","text":"Once the DataLoaders and the Faster RCNN model are created, we create the fastai Learner. The latter uses the DataLoaders and the Faster RCNN model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the Faster RCNN model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Example"},{"location":"model_faster_rcnn/#pytorch-lightning-example","text":"The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same Faster RCNN model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the Faster RCNN model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Pytorch-Lightning Example"},{"location":"model_faster_rcnn/#how-to-train-the-pets-dataset-using-faster-rcnn","text":"Source Code","title":"How to train the PETS Dataset using Faster RCNN"},{"location":"model_faster_rcnn/#paper-introduction","text":"Faster R-CNN is built upon the knowledge of Fast RCNN which indeed built upon the ideas of RCNN and SPP-Net. In their paper, the authors introduced a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.","title":"Paper Introduction"},{"location":"model_faster_rcnn/#references","text":"","title":"References"},{"location":"model_faster_rcnn/#paper","text":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks","title":"Paper"},{"location":"model_faster_rcnn/#blog-posts","text":"An overview of deep-learning based object-detection algorithms Guide to build Faster RCNN in PyTorch","title":"Blog Posts"},{"location":"model_faster_rcnn/#torchvision-implementation","text":"Torchvision Object Detection Finetuning Tutorial Torchvision Faster RCNN Implementation","title":"Torchvision Implementation"},{"location":"negative_samples/","text":"How to use negative examples In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons! Info The first half of this tutorial is almost an exact copy from this tutorial. Installation ! pip install icevision [ all ] icedata Imports from icevision.all import * Define class_map Even when training with pets data the only class we want to detect is raccoon . class_map = ClassMap ([ 'raccoon' ]) Raccoon dataset The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . voc ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' , class_map = class_map ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map ) Pets dataset With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that returns an empty list for labels and bboxes . Remember the steps for generating a custom parser (check this tutorial for more information), first define all your mixins and call generate_template : class PetsImageParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): pass PetsImageParser . generate_template () def __iter__(self) -> Any: def imageid(self, o) -> Hashable: def image_width_height(self, o) -> Tuple[int, int]: return get_image_size(self.filepath(o)) def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: And now we use that to fill the required methods: class PetsImageParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): def __init__ ( self , data_dir ): self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def imageid ( self , o ) -> Hashable : return o . stem def filepath ( self , o ) -> Union [ str , Path ]: return o def image_width_height ( self , o ) -> Tuple [ int , int ]: return get_image_size ( self . filepath ( o )) def labels ( self , o ) -> List [ int ]: return [] def bboxes ( self , o ) -> List [ BBox ]: return [] Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 , class_map = class_map ) Transforms Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) Datasets and DataLoaders We create the raccoon dataset and dataloader as normal: raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = efficientdet . train_dl ( raccoon_train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) raccoon_valid_dl = efficientdet . valid_dl ( raccoon_valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = efficientdet . train_dl ( combined_train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) combined_valid_dl = efficientdet . valid_dl ( combined_valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 ) Metrics As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Models We're now ready to train a separate model for each dataset and see how the results change! Raccoons only raccoon_model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) raccoon_learn = efficientdet . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 280.362213 288.541992 0.000031 00:09 1 255.560837 275.054901 0.001539 00:07 2 201.386978 203.328262 0.000882 00:07 3 138.040710 53.258076 0.000992 00:07 4 98.638695 21.866730 0.185121 00:07 epoch train_loss valid_loss COCOMetric time 0 0.789586 23.308876 0.220375 00:08 1 0.738028 13.141761 0.234339 00:08 2 0.692019 9.430372 0.289227 00:08 3 0.646733 5.213285 0.367285 00:08 4 0.598996 2.703029 0.499523 00:08 5 0.566802 2.002367 0.428408 00:08 6 0.540306 1.171565 0.512689 00:08 7 0.514873 1.075465 0.408631 00:08 8 0.494372 0.699056 0.547269 00:08 9 0.474496 0.772929 0.429607 00:08 10 0.456045 0.565817 0.545960 00:08 11 0.436963 0.516108 0.502583 00:08 12 0.422333 0.526793 0.552966 00:08 13 0.406885 0.607857 0.435570 00:08 14 0.396543 0.450901 0.538114 00:08 15 0.385096 0.433830 0.568332 00:08 16 0.372637 0.433433 0.556526 00:08 17 0.356291 0.412439 0.615498 00:08 18 0.346603 0.424790 0.583552 00:08 19 0.335853 0.394423 0.599287 00:08 20 0.323742 0.351422 0.660224 00:08 21 0.314364 0.338674 0.640041 00:08 22 0.307630 0.344830 0.655402 00:08 23 0.303521 0.322497 0.656412 00:08 24 0.297675 0.319093 0.654938 00:08 25 0.288858 0.333945 0.641190 00:08 26 0.286353 0.320516 0.664691 00:08 27 0.282811 0.305629 0.669700 00:08 28 0.280102 0.300712 0.678493 00:08 29 0.273880 0.301367 0.671941 00:08 If only raccoon photos are showed during training, everything is a raccoon! efficientdet . show_results ( raccoon_model , combined_valid_ds , class_map = class_map ) Raccoons + pets combined_model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) combined_learn = efficientdet . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 426.231720 3113.471436 0.000021 00:12 1 344.480865 2650.498291 0.000036 00:11 2 206.628662 606.958130 0.001016 00:11 3 126.901100 243.355576 0.056071 00:11 4 82.448853 109.352119 0.030934 00:10 epoch train_loss valid_loss COCOMetric time 0 1.062148 51.029591 0.089626 00:12 1 0.927982 24.122263 0.161439 00:12 2 0.834056 10.649617 0.218092 00:12 3 0.797795 11.055972 0.274900 00:12 4 0.765396 7.913944 0.280049 00:12 5 0.727915 3.130997 0.271282 00:12 6 0.714074 4.496913 0.217573 00:12 7 0.713696 3.117041 0.384929 00:12 8 0.671189 1.860370 0.311723 00:12 9 0.637322 1.757796 0.406194 00:12 10 0.611760 2.911715 0.369743 00:12 11 0.590360 6.035703 0.444911 00:12 12 0.574620 2.676531 0.419094 00:12 13 0.547478 4.088236 0.514613 00:12 14 0.518071 3.934689 0.492547 00:12 15 0.501104 2.299222 0.502294 00:12 16 0.476237 3.566992 0.424591 00:12 17 0.451466 3.532001 0.524310 00:12 18 0.437068 2.166881 0.551229 00:12 19 0.406420 2.166020 0.590745 00:12 20 0.383731 2.122064 0.548815 00:12 21 0.378371 2.332926 0.583251 00:12 22 0.381634 1.484042 0.551595 00:12 23 0.374729 2.162852 0.594631 00:12 24 0.364782 2.040329 0.628670 00:12 25 0.348715 1.812243 0.597921 00:12 26 0.335386 1.620427 0.630255 00:12 27 0.322967 1.752403 0.625841 00:12 28 0.327148 1.533834 0.629842 00:12 29 0.323913 1.508479 0.627119 00:12 When negative samples are used during training, the model get's way better understading what is not a raccoon. efficientdet . show_results ( combined_model , combined_valid_ds , class_map = class_map ) Happy Learning! That's it folks! If you need any assistance, feel free to join our forum .","title":"How to use negative samples"},{"location":"negative_samples/#how-to-use-negative-examples","text":"In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons! Info The first half of this tutorial is almost an exact copy from this tutorial.","title":"How to use negative examples"},{"location":"negative_samples/#installation","text":"! pip install icevision [ all ] icedata","title":"Installation"},{"location":"negative_samples/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"negative_samples/#define-class_map","text":"Even when training with pets data the only class we want to detect is raccoon . class_map = ClassMap ([ 'raccoon' ])","title":"Define class_map"},{"location":"negative_samples/#raccoon-dataset","text":"The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . voc ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' , class_map = class_map ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map )","title":"Raccoon dataset"},{"location":"negative_samples/#pets-dataset","text":"With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that returns an empty list for labels and bboxes . Remember the steps for generating a custom parser (check this tutorial for more information), first define all your mixins and call generate_template : class PetsImageParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): pass PetsImageParser . generate_template () def __iter__(self) -> Any: def imageid(self, o) -> Hashable: def image_width_height(self, o) -> Tuple[int, int]: return get_image_size(self.filepath(o)) def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: And now we use that to fill the required methods: class PetsImageParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixin ): def __init__ ( self , data_dir ): self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def imageid ( self , o ) -> Hashable : return o . stem def filepath ( self , o ) -> Union [ str , Path ]: return o def image_width_height ( self , o ) -> Tuple [ int , int ]: return get_image_size ( self . filepath ( o )) def labels ( self , o ) -> List [ int ]: return [] def bboxes ( self , o ) -> List [ BBox ]: return [] Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 , class_map = class_map )","title":"Pets dataset"},{"location":"negative_samples/#transforms","text":"Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"negative_samples/#datasets-and-dataloaders","text":"We create the raccoon dataset and dataloader as normal: raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = efficientdet . train_dl ( raccoon_train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) raccoon_valid_dl = efficientdet . valid_dl ( raccoon_valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = efficientdet . train_dl ( combined_train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) combined_valid_dl = efficientdet . valid_dl ( combined_valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 )","title":"Datasets and DataLoaders"},{"location":"negative_samples/#metrics","text":"As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"negative_samples/#models","text":"We're now ready to train a separate model for each dataset and see how the results change!","title":"Models"},{"location":"negative_samples/#raccoons-only","text":"raccoon_model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) raccoon_learn = efficientdet . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 280.362213 288.541992 0.000031 00:09 1 255.560837 275.054901 0.001539 00:07 2 201.386978 203.328262 0.000882 00:07 3 138.040710 53.258076 0.000992 00:07 4 98.638695 21.866730 0.185121 00:07 epoch train_loss valid_loss COCOMetric time 0 0.789586 23.308876 0.220375 00:08 1 0.738028 13.141761 0.234339 00:08 2 0.692019 9.430372 0.289227 00:08 3 0.646733 5.213285 0.367285 00:08 4 0.598996 2.703029 0.499523 00:08 5 0.566802 2.002367 0.428408 00:08 6 0.540306 1.171565 0.512689 00:08 7 0.514873 1.075465 0.408631 00:08 8 0.494372 0.699056 0.547269 00:08 9 0.474496 0.772929 0.429607 00:08 10 0.456045 0.565817 0.545960 00:08 11 0.436963 0.516108 0.502583 00:08 12 0.422333 0.526793 0.552966 00:08 13 0.406885 0.607857 0.435570 00:08 14 0.396543 0.450901 0.538114 00:08 15 0.385096 0.433830 0.568332 00:08 16 0.372637 0.433433 0.556526 00:08 17 0.356291 0.412439 0.615498 00:08 18 0.346603 0.424790 0.583552 00:08 19 0.335853 0.394423 0.599287 00:08 20 0.323742 0.351422 0.660224 00:08 21 0.314364 0.338674 0.640041 00:08 22 0.307630 0.344830 0.655402 00:08 23 0.303521 0.322497 0.656412 00:08 24 0.297675 0.319093 0.654938 00:08 25 0.288858 0.333945 0.641190 00:08 26 0.286353 0.320516 0.664691 00:08 27 0.282811 0.305629 0.669700 00:08 28 0.280102 0.300712 0.678493 00:08 29 0.273880 0.301367 0.671941 00:08 If only raccoon photos are showed during training, everything is a raccoon! efficientdet . show_results ( raccoon_model , combined_valid_ds , class_map = class_map )","title":"Raccoons only"},{"location":"negative_samples/#raccoons-pets","text":"combined_model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) combined_learn = efficientdet . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 426.231720 3113.471436 0.000021 00:12 1 344.480865 2650.498291 0.000036 00:11 2 206.628662 606.958130 0.001016 00:11 3 126.901100 243.355576 0.056071 00:11 4 82.448853 109.352119 0.030934 00:10 epoch train_loss valid_loss COCOMetric time 0 1.062148 51.029591 0.089626 00:12 1 0.927982 24.122263 0.161439 00:12 2 0.834056 10.649617 0.218092 00:12 3 0.797795 11.055972 0.274900 00:12 4 0.765396 7.913944 0.280049 00:12 5 0.727915 3.130997 0.271282 00:12 6 0.714074 4.496913 0.217573 00:12 7 0.713696 3.117041 0.384929 00:12 8 0.671189 1.860370 0.311723 00:12 9 0.637322 1.757796 0.406194 00:12 10 0.611760 2.911715 0.369743 00:12 11 0.590360 6.035703 0.444911 00:12 12 0.574620 2.676531 0.419094 00:12 13 0.547478 4.088236 0.514613 00:12 14 0.518071 3.934689 0.492547 00:12 15 0.501104 2.299222 0.502294 00:12 16 0.476237 3.566992 0.424591 00:12 17 0.451466 3.532001 0.524310 00:12 18 0.437068 2.166881 0.551229 00:12 19 0.406420 2.166020 0.590745 00:12 20 0.383731 2.122064 0.548815 00:12 21 0.378371 2.332926 0.583251 00:12 22 0.381634 1.484042 0.551595 00:12 23 0.374729 2.162852 0.594631 00:12 24 0.364782 2.040329 0.628670 00:12 25 0.348715 1.812243 0.597921 00:12 26 0.335386 1.620427 0.630255 00:12 27 0.322967 1.752403 0.625841 00:12 28 0.327148 1.533834 0.629842 00:12 29 0.323913 1.508479 0.627119 00:12 When negative samples are used during training, the model get's way better understading what is not a raccoon. efficientdet . show_results ( combined_model , combined_valid_ds , class_map = class_map )","title":"Raccoons + pets"},{"location":"negative_samples/#happy-learning","text":"That's it folks! If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"ochuman_keypoint_detection/","text":"OCHuman dataset From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study. Installing IceVision !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade -q !pip install git+git://github.com/airctic/icedata.git -q from google.colab import drive drive.mount('/content/gdrive') Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True). Defining OCHuman parser from icevision.all import * _ = icedata.ochuman.load_data() \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m11\u001b[0m Parse data parser = icedata.ochuman.parser(\"/content/gdrive/My Drive/icevision/OCHuman/ochuman.json\", \"/content/gdrive/My Drive/icevision/OCHuman/images/images/\") train_records, valid_records = parser.parse(data_splitter=RandomSplitter([0.8, 0.2]), cache_filepath=\"/content/gdrive/My Drive/icevision/OCHuman/ochuman.pkl\") \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from /content/gdrive/My Drive/icevision/OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m115\u001b[0m Datasets + augmentations presize = 384 size = 224 valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize, crop_fn=None), tfms.A.Normalize()]) train_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) samples = [train_ds[1] for _ in range(6)] show_samples(samples, ncols=3) len(train_ds), len(valid_ds) (4064, 1017) Dataloaders train_dl = keypoint_rcnn.train_dl(train_ds, batch_size=32, num_workers=4, shuffle=True) valid_dl = keypoint_rcnn.valid_dl(train_ds, batch_size=32, num_workers=4, shuffle=False) Model model = keypoint_rcnn.model(num_keypoints=19) Downloading: \"https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\" to /root/.cache/torch/hub/checkpoints/keypointrcnn_resnet50_fpn_coco-fc266e95.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=237034793.0), HTML(value=''))) Train a fastai learner learn = keypoint_rcnn.fastai.learner(dls=[train_dl, valid_dl], model=model) learn.lr_find() SuggestedLRs(lr_min=0.0001737800776027143, lr_steep=0.0002290867705596611) learn.fine_tune(5, 1e-4, freeze_epochs=3) epoch train_loss valid_loss time 0 6.535554 5.772009 25:12 1 5.475295 5.279093 17:55 2 5.220623 5.119839 14:52 epoch train_loss valid_loss time 0 4.984282 4.943438 19:19 1 4.936190 4.892380 19:38 2 4.886095 4.841107 18:31 3 4.821804 4.805563 17:05 4 4.787708 4.789882 16:31 learn.lr_find() SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=1.0964781722577754e-06) learn.fine_tune(5, 1e-5, freeze_epochs=0) epoch train_loss valid_loss time 0 4.794446 4.788306 15:49 1 4.798432 4.785392 15:11 2 4.771396 4.779112 15:45 3 4.787804 4.784240 15:03 4 4.783122 4.780210 15:31 Show model results keypoint_rcnn.show_results(model, valid_ds) Save model torch.save(model.state_dict(), \"/content/gdrive/My Drive/icevision/OCHuman/model.pth\") model = keypoint_rcnn.model(num_keypoints=19) state_dict = torch.load(\"/content/gdrive/My Drive/icevision/OCHuman/model.pth\") model.load_state_dict(state_dict) <All keys matched successfully> Running inference on validation set infer_dl = keypoint_rcnn.infer_dl(valid_ds, batch_size=8) samples, preds = keypoint_rcnn.predict_dl(model=model, infer_dl=infer_dl) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) preds[1] show_preds(samples=samples[68:70], preds=preds[68:70], show=True, display_label=False, figsize=(10, 10)) {'above_threshold': tensor([True]), 'bboxes': [<BBox (xmin:56.87401580810547, ymin:11.817001342773438, xmax:175.61013793945312, ymax:219.280029296875)>], 'keypoints': [<KeyPoints (19 visible keypoints)>], 'keypoints_scores': array([[ 6.3175044 , 4.77172 , 5.2034507 , 6.2616506 , 5.187912 , 5.0277104 , 4.90092 , 2.5671883 , -0.09820518, 4.783924 , 2.211214 , -0.6368867 , 8.511393 , 9.750954 , 10.703004 , 9.765849 , 12.346814 , 10.481525 , 10.295473 ]], dtype=float32), 'labels': array([1]), 'scores': array([0.9997154], dtype=float32)}","title":"Ochuman keypoint detection"},{"location":"ochuman_keypoint_detection/#ochuman-dataset","text":"From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study.","title":"OCHuman dataset"},{"location":"ochuman_keypoint_detection/#installing-icevision","text":"!pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade -q !pip install git+git://github.com/airctic/icedata.git -q from google.colab import drive drive.mount('/content/gdrive') Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).","title":"Installing IceVision"},{"location":"ochuman_keypoint_detection/#defining-ochuman-parser","text":"from icevision.all import * _ = icedata.ochuman.load_data() \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m11\u001b[0m","title":"Defining OCHuman parser"},{"location":"ochuman_keypoint_detection/#parse-data","text":"parser = icedata.ochuman.parser(\"/content/gdrive/My Drive/icevision/OCHuman/ochuman.json\", \"/content/gdrive/My Drive/icevision/OCHuman/images/images/\") train_records, valid_records = parser.parse(data_splitter=RandomSplitter([0.8, 0.2]), cache_filepath=\"/content/gdrive/My Drive/icevision/OCHuman/ochuman.pkl\") \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from /content/gdrive/My Drive/icevision/OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m115\u001b[0m","title":"Parse data"},{"location":"ochuman_keypoint_detection/#datasets-augmentations","text":"presize = 384 size = 224 valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize, crop_fn=None), tfms.A.Normalize()]) train_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) samples = [train_ds[1] for _ in range(6)] show_samples(samples, ncols=3) len(train_ds), len(valid_ds) (4064, 1017)","title":"Datasets + augmentations"},{"location":"ochuman_keypoint_detection/#dataloaders","text":"train_dl = keypoint_rcnn.train_dl(train_ds, batch_size=32, num_workers=4, shuffle=True) valid_dl = keypoint_rcnn.valid_dl(train_ds, batch_size=32, num_workers=4, shuffle=False)","title":"Dataloaders"},{"location":"ochuman_keypoint_detection/#model","text":"model = keypoint_rcnn.model(num_keypoints=19) Downloading: \"https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\" to /root/.cache/torch/hub/checkpoints/keypointrcnn_resnet50_fpn_coco-fc266e95.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=237034793.0), HTML(value='')))","title":"Model"},{"location":"ochuman_keypoint_detection/#train-a-fastai-learner","text":"learn = keypoint_rcnn.fastai.learner(dls=[train_dl, valid_dl], model=model) learn.lr_find() SuggestedLRs(lr_min=0.0001737800776027143, lr_steep=0.0002290867705596611) learn.fine_tune(5, 1e-4, freeze_epochs=3) epoch train_loss valid_loss time 0 6.535554 5.772009 25:12 1 5.475295 5.279093 17:55 2 5.220623 5.119839 14:52 epoch train_loss valid_loss time 0 4.984282 4.943438 19:19 1 4.936190 4.892380 19:38 2 4.886095 4.841107 18:31 3 4.821804 4.805563 17:05 4 4.787708 4.789882 16:31 learn.lr_find() SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=1.0964781722577754e-06) learn.fine_tune(5, 1e-5, freeze_epochs=0) epoch train_loss valid_loss time 0 4.794446 4.788306 15:49 1 4.798432 4.785392 15:11 2 4.771396 4.779112 15:45 3 4.787804 4.784240 15:03 4 4.783122 4.780210 15:31","title":"Train a fastai learner"},{"location":"ochuman_keypoint_detection/#show-model-results","text":"keypoint_rcnn.show_results(model, valid_ds)","title":"Show model results"},{"location":"ochuman_keypoint_detection/#save-model","text":"torch.save(model.state_dict(), \"/content/gdrive/My Drive/icevision/OCHuman/model.pth\") model = keypoint_rcnn.model(num_keypoints=19) state_dict = torch.load(\"/content/gdrive/My Drive/icevision/OCHuman/model.pth\") model.load_state_dict(state_dict) <All keys matched successfully>","title":"Save model"},{"location":"ochuman_keypoint_detection/#running-inference-on-validation-set","text":"infer_dl = keypoint_rcnn.infer_dl(valid_ds, batch_size=8) samples, preds = keypoint_rcnn.predict_dl(model=model, infer_dl=infer_dl) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) preds[1] show_preds(samples=samples[68:70], preds=preds[68:70], show=True, display_label=False, figsize=(10, 10)) {'above_threshold': tensor([True]), 'bboxes': [<BBox (xmin:56.87401580810547, ymin:11.817001342773438, xmax:175.61013793945312, ymax:219.280029296875)>], 'keypoints': [<KeyPoints (19 visible keypoints)>], 'keypoints_scores': array([[ 6.3175044 , 4.77172 , 5.2034507 , 6.2616506 , 5.187912 , 5.0277104 , 4.90092 , 2.5671883 , -0.09820518, 4.783924 , 2.211214 , -0.6368867 , 8.511393 , 9.750954 , 10.703004 , 9.765849 , 12.346814 , 10.481525 , 10.295473 ]], dtype=float32), 'labels': array([1]), 'scores': array([0.9997154], dtype=float32)}","title":"Running inference on validation set"},{"location":"parser/","text":"[source] Parser icevision . parsers . Parser ( * args , ** kwargs ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source] parse Parser . parse ( data_splitter = None , idmap = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. idmap icevision.core.id_map.IDMap : Maps from filenames to unique ids, pass an IDMap() if you need this information. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter . [source] FasterRCNN icevision . parsers . FasterRCNN ( * args , ** kwargs ) Parser with required mixins for Faster RCNN. [source] MaskRCNN icevision . parsers . MaskRCNN ( * args , ** kwargs ) Parser with required mixins for Mask RCNN. [source] ImageidMixin icevision . parsers . mixins . ImageidMixin ( * args , ** kwargs ) Adds imageid method to parser [source] FilepathMixin icevision . parsers . mixins . FilepathMixin ( * args , ** kwargs ) Adds filepath method to parser [source] SizeMixin icevision . parsers . mixins . SizeMixin ( * args , ** kwargs ) Adds image_width_height method to parser [source] LabelsMixin icevision . parsers . mixins . LabelsMixin ( * args , ** kwargs ) Adds labels method to parser [source] BBoxesMixin icevision . parsers . mixins . BBoxesMixin ( * args , ** kwargs ) Adds bboxes method to parser [source] MasksMixin icevision . parsers . mixins . MasksMixin ( * args , ** kwargs ) Adds masks method to parser [source] AreasMixin icevision . parsers . mixins . AreasMixin ( * args , ** kwargs ) Adds areas method to parser [source] IsCrowdsMixin icevision . parsers . mixins . IsCrowdsMixin ( * args , ** kwargs ) Adds iscrowds method to parser","title":"Parser"},{"location":"parser/#parser","text":"icevision . parsers . Parser ( * args , ** kwargs ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source]","title":"Parser"},{"location":"parser/#parse","text":"Parser . parse ( data_splitter = None , idmap = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. idmap icevision.core.id_map.IDMap : Maps from filenames to unique ids, pass an IDMap() if you need this information. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter . [source]","title":"parse"},{"location":"parser/#fasterrcnn","text":"icevision . parsers . FasterRCNN ( * args , ** kwargs ) Parser with required mixins for Faster RCNN. [source]","title":"FasterRCNN"},{"location":"parser/#maskrcnn","text":"icevision . parsers . MaskRCNN ( * args , ** kwargs ) Parser with required mixins for Mask RCNN. [source]","title":"MaskRCNN"},{"location":"parser/#imageidmixin","text":"icevision . parsers . mixins . ImageidMixin ( * args , ** kwargs ) Adds imageid method to parser [source]","title":"ImageidMixin"},{"location":"parser/#filepathmixin","text":"icevision . parsers . mixins . FilepathMixin ( * args , ** kwargs ) Adds filepath method to parser [source]","title":"FilepathMixin"},{"location":"parser/#sizemixin","text":"icevision . parsers . mixins . SizeMixin ( * args , ** kwargs ) Adds image_width_height method to parser [source]","title":"SizeMixin"},{"location":"parser/#labelsmixin","text":"icevision . parsers . mixins . LabelsMixin ( * args , ** kwargs ) Adds labels method to parser [source]","title":"LabelsMixin"},{"location":"parser/#bboxesmixin","text":"icevision . parsers . mixins . BBoxesMixin ( * args , ** kwargs ) Adds bboxes method to parser [source]","title":"BBoxesMixin"},{"location":"parser/#masksmixin","text":"icevision . parsers . mixins . MasksMixin ( * args , ** kwargs ) Adds masks method to parser [source]","title":"MasksMixin"},{"location":"parser/#areasmixin","text":"icevision . parsers . mixins . AreasMixin ( * args , ** kwargs ) Adds areas method to parser [source]","title":"AreasMixin"},{"location":"parser/#iscrowdsmixin","text":"icevision . parsers . mixins . IsCrowdsMixin ( * args , ** kwargs ) Adds iscrowds method to parser","title":"IsCrowdsMixin"},{"location":"plot_top_losses/","text":"The purpose of this notebook is to showcase the newly added plot_top_losses functionality, which allows users to inspect models' results by plotting images sorted by various combinations of losses. This API makes it easy to immediately spot pictures the model struggles the most with, giving the practitioner the opportunity to take swift action to correct this behaviour (remove wrong samples, correct mis-labellings, etc). plot_top_losses is available for all IceVision models, as the below notebook shows. Install IceVision ! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade Object Detection Load fridge dataset from icevision.all import * # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False ) Train faster_rcnn model model = faster_rcnn . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=167502836.0), HTML(value=''))) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 5 , 1e-2 , freeze_epochs = 3 ) epoch train_loss valid_loss COCOMetric time 0 0.929798 0.745340 0.110503 00:09 1 0.801614 0.534930 0.287367 00:06 2 0.683533 0.503273 0.254913 00:07 epoch train_loss valid_loss COCOMetric time 0 4.777527 0.727937 0.112012 00:11 1 2.675926 0.900404 0.087199 00:09 2 1.949393 0.676135 0.063974 00:09 3 1.550345 0.538352 0.075615 00:09 4 1.305145 0.539530 0.101115 00:09 Run top_plot_losses on faster_rcnn model results Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_box_reg': {'1ile': 0.05190576612949371, '25ile': 0.2035452425479889, '50ile': 0.2641611248254776, '75ile': 0.3478182256221771, '99ile': 0.5161057710647583, 'max': 0.5161057710647583, 'mean': 0.2718363240934335, 'min': 0.05190576612949371}, 'loss_classifier': {'1ile': 0.0807231068611145, '25ile': 0.1814981997013092, '50ile': 0.25537528097629547, '75ile': 0.32677075266838074, '99ile': 0.4015304148197174, 'max': 0.4015304148197174, 'mean': 0.24639798099031815, 'min': 0.0807231068611145}, 'loss_objectness': {'1ile': 5.380709990276955e-05, '25ile': 0.0020654327236115932, '50ile': 0.006522084586322308, '75ile': 0.010207928717136383, '99ile': 0.030636457726359367, 'max': 0.030636457726359367, 'mean': 0.007866157029499075, 'min': 5.380709990276955e-05}, 'loss_rpn_box_reg': {'1ile': 0.0014084185240790248, '25ile': 0.008392253890633583, '50ile': 0.014501482248306274, '75ile': 0.017447534948587418, '99ile': 0.03517897427082062, 'max': 0.03517897427082062, 'mean': 0.013996926015422035, 'min': 0.0014084185240790248}, 'loss_total': {'1ile': 0.13555445312522352, '25ile': 0.4158565173856914, '50ile': 0.5547791894059628, '75ile': 0.7083070203661919, '99ile': 0.9351915549486876, 'max': 0.9351915549486876, 'mean': 0.5400973881286728, 'min': 0.13555445312522352}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filepath loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted 0 /root/.icevision/data/fridge/odFridgeObjects/images/70.jpg 0.401530 0.516106 0.003187 0.014368 0.935192 0.233798 1 /root/.icevision/data/fridge/odFridgeObjects/images/65.jpg 0.361605 0.396446 0.004368 0.035179 0.797597 0.199399 2 /root/.icevision/data/fridge/odFridgeObjects/images/69.jpg 0.325730 0.436458 0.007038 0.021536 0.790761 0.197690 3 /root/.icevision/data/fridge/odFridgeObjects/images/82.jpg 0.336966 0.380997 0.030636 0.014635 0.763234 0.190809 4 /root/.icevision/data/fridge/odFridgeObjects/images/76.jpg 0.344810 0.397597 0.007009 0.011474 0.760891 0.190223 5 /root/.icevision/data/fridge/odFridgeObjects/images/91.jpg 0.347157 0.377784 0.013784 0.014169 0.752894 0.188223 6 /root/.icevision/data/fridge/odFridgeObjects/images/73.jpg 0.334780 0.347818 0.008493 0.017216 0.708307 0.177077 7 /root/.icevision/data/fridge/odFridgeObjects/images/63.jpg 0.326771 0.343447 0.003026 0.020271 0.693515 0.173379 8 /root/.icevision/data/fridge/odFridgeObjects/images/125.jpg 0.284659 0.318850 0.008986 0.012640 0.625135 0.156284 9 /root/.icevision/data/fridge/odFridgeObjects/images/51.jpg 0.253484 0.333823 0.006035 0.008696 0.602038 0.150509 10 /root/.icevision/data/fridge/odFridgeObjects/images/66.jpg 0.282760 0.277893 0.008295 0.024600 0.593548 0.148387 11 /root/.icevision/data/fridge/odFridgeObjects/images/42.jpg 0.257737 0.299093 0.016960 0.017189 0.590978 0.147745 12 /root/.icevision/data/fridge/odFridgeObjects/images/59.jpg 0.257267 0.262500 0.021031 0.022185 0.562983 0.140746 13 /root/.icevision/data/fridge/odFridgeObjects/images/90.jpg 0.260042 0.265823 0.004306 0.016405 0.546575 0.136644 14 /root/.icevision/data/fridge/odFridgeObjects/images/109.jpg 0.248338 0.260444 0.018180 0.017427 0.544389 0.136097 15 /root/.icevision/data/fridge/odFridgeObjects/images/16.jpg 0.221121 0.255459 0.002065 0.005470 0.484115 0.121029 16 /root/.icevision/data/fridge/odFridgeObjects/images/28.jpg 0.228037 0.208443 0.013820 0.018261 0.468562 0.117141 17 /root/.icevision/data/fridge/odFridgeObjects/images/80.jpg 0.218763 0.217823 0.007633 0.017448 0.461667 0.115417 18 /root/.icevision/data/fridge/odFridgeObjects/images/33.jpg 0.211880 0.199396 0.010208 0.016371 0.437855 0.109464 19 /root/.icevision/data/fridge/odFridgeObjects/images/123.jpg 0.181498 0.219479 0.001994 0.012886 0.415857 0.103964 20 /root/.icevision/data/fridge/odFridgeObjects/images/100.jpg 0.121183 0.203545 0.000172 0.001678 0.326578 0.081645 21 /root/.icevision/data/fridge/odFridgeObjects/images/102.jpg 0.141722 0.161956 0.000054 0.002683 0.306415 0.076604 22 /root/.icevision/data/fridge/odFridgeObjects/images/19.jpg 0.129916 0.137039 0.000416 0.007992 0.275363 0.068841 23 /root/.icevision/data/fridge/odFridgeObjects/images/3.jpg 0.134473 0.104869 0.005156 0.008392 0.252891 0.063223 24 /root/.icevision/data/fridge/odFridgeObjects/images/4.jpg 0.113395 0.092752 0.000150 0.003340 0.209638 0.052409 25 /root/.icevision/data/fridge/odFridgeObjects/images/1.jpg 0.080723 0.051906 0.001517 0.001408 0.135554 0.033889 Run top_plot_losses on a retinanet pretrained (but not finetuned) model model = retinanet . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=136595076.0), HTML(value=''))) by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_classification\" : 0.25 , \"loss_bbox_regression\" : 0.75 , } } sorted_samples , sorted_preds , losses_stats = retinanet . interp . plot_top_losses ( model , valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classification', 'loss_bbox_regression']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) Run top_plot_losses on a efficientdet pretrained (but not finetuned) model model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = 384 ) sorted_samples , sorted_preds , losses_stats = efficientdet . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) Instance Segmentation plot_top_losses in action with a mask_rcnn model on the pennfudan dataset data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=53723336.0), HTML(value=''))) parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 1 , shuffle = True , num_workers = 0 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 1 , shuffle = False , num_workers = 0 ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m model = mask_rcnn . model ( num_classes = len ( class_map )) learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178090079.0), HTML(value=''))) epoch train_loss valid_loss time 0 0.702523 0.555716 00:18 1 0.657636 0.522894 00:14 epoch train_loss valid_loss time 0 0.551434 0.377621 00:20 1 0.469830 0.448473 00:19 2 0.472173 0.364438 00:19 3 0.437653 0.338397 00:19 4 0.375957 0.342569 00:19 5 0.385508 0.317213 00:20 6 0.371840 0.343802 00:19 7 0.356101 0.358258 00:19 8 0.484352 0.322044 00:19 9 0.339008 0.312049 00:19 sorted_samples , sorted_preds , losses_stats = mask_rcnn . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=34.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value=''))) Keypoint Detection plot_top_losses in action with a keypoint_rcnn model on the biwi dataset data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = keypoint_rcnn . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = keypoint_rcnn . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=593774.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = keypoint_rcnn . model ( backbone = backbone , num_keypoints = 1 ) Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value=''))) learn = keypoint_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss time 0 9.269516 8.537302 00:35 1 8.527759 7.503790 00:16 epoch train_loss valid_loss time 0 7.033783 6.651293 00:22 1 6.630652 5.782725 00:24 2 6.220629 5.254831 00:15 3 5.921734 5.051409 00:19 4 5.713860 4.953146 00:17 sorted_samples , sorted_preds , losses_stats = keypoint_rcnn . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=40.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))","title":"Plot top losses"},{"location":"plot_top_losses/#install-icevision","text":"! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade","title":"Install IceVision"},{"location":"plot_top_losses/#object-detection","text":"","title":"Object Detection"},{"location":"plot_top_losses/#load-fridge-dataset","text":"from icevision.all import * # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False )","title":"Load fridge dataset"},{"location":"plot_top_losses/#train-faster_rcnn-model","text":"model = faster_rcnn . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=167502836.0), HTML(value=''))) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 5 , 1e-2 , freeze_epochs = 3 ) epoch train_loss valid_loss COCOMetric time 0 0.929798 0.745340 0.110503 00:09 1 0.801614 0.534930 0.287367 00:06 2 0.683533 0.503273 0.254913 00:07 epoch train_loss valid_loss COCOMetric time 0 4.777527 0.727937 0.112012 00:11 1 2.675926 0.900404 0.087199 00:09 2 1.949393 0.676135 0.063974 00:09 3 1.550345 0.538352 0.075615 00:09 4 1.305145 0.539530 0.101115 00:09","title":"Train faster_rcnn model"},{"location":"plot_top_losses/#run-top_plot_losses-on-faster_rcnn-model-results","text":"Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = faster_rcnn . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_box_reg': {'1ile': 0.05190576612949371, '25ile': 0.2035452425479889, '50ile': 0.2641611248254776, '75ile': 0.3478182256221771, '99ile': 0.5161057710647583, 'max': 0.5161057710647583, 'mean': 0.2718363240934335, 'min': 0.05190576612949371}, 'loss_classifier': {'1ile': 0.0807231068611145, '25ile': 0.1814981997013092, '50ile': 0.25537528097629547, '75ile': 0.32677075266838074, '99ile': 0.4015304148197174, 'max': 0.4015304148197174, 'mean': 0.24639798099031815, 'min': 0.0807231068611145}, 'loss_objectness': {'1ile': 5.380709990276955e-05, '25ile': 0.0020654327236115932, '50ile': 0.006522084586322308, '75ile': 0.010207928717136383, '99ile': 0.030636457726359367, 'max': 0.030636457726359367, 'mean': 0.007866157029499075, 'min': 5.380709990276955e-05}, 'loss_rpn_box_reg': {'1ile': 0.0014084185240790248, '25ile': 0.008392253890633583, '50ile': 0.014501482248306274, '75ile': 0.017447534948587418, '99ile': 0.03517897427082062, 'max': 0.03517897427082062, 'mean': 0.013996926015422035, 'min': 0.0014084185240790248}, 'loss_total': {'1ile': 0.13555445312522352, '25ile': 0.4158565173856914, '50ile': 0.5547791894059628, '75ile': 0.7083070203661919, '99ile': 0.9351915549486876, 'max': 0.9351915549486876, 'mean': 0.5400973881286728, 'min': 0.13555445312522352}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filepath loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted 0 /root/.icevision/data/fridge/odFridgeObjects/images/70.jpg 0.401530 0.516106 0.003187 0.014368 0.935192 0.233798 1 /root/.icevision/data/fridge/odFridgeObjects/images/65.jpg 0.361605 0.396446 0.004368 0.035179 0.797597 0.199399 2 /root/.icevision/data/fridge/odFridgeObjects/images/69.jpg 0.325730 0.436458 0.007038 0.021536 0.790761 0.197690 3 /root/.icevision/data/fridge/odFridgeObjects/images/82.jpg 0.336966 0.380997 0.030636 0.014635 0.763234 0.190809 4 /root/.icevision/data/fridge/odFridgeObjects/images/76.jpg 0.344810 0.397597 0.007009 0.011474 0.760891 0.190223 5 /root/.icevision/data/fridge/odFridgeObjects/images/91.jpg 0.347157 0.377784 0.013784 0.014169 0.752894 0.188223 6 /root/.icevision/data/fridge/odFridgeObjects/images/73.jpg 0.334780 0.347818 0.008493 0.017216 0.708307 0.177077 7 /root/.icevision/data/fridge/odFridgeObjects/images/63.jpg 0.326771 0.343447 0.003026 0.020271 0.693515 0.173379 8 /root/.icevision/data/fridge/odFridgeObjects/images/125.jpg 0.284659 0.318850 0.008986 0.012640 0.625135 0.156284 9 /root/.icevision/data/fridge/odFridgeObjects/images/51.jpg 0.253484 0.333823 0.006035 0.008696 0.602038 0.150509 10 /root/.icevision/data/fridge/odFridgeObjects/images/66.jpg 0.282760 0.277893 0.008295 0.024600 0.593548 0.148387 11 /root/.icevision/data/fridge/odFridgeObjects/images/42.jpg 0.257737 0.299093 0.016960 0.017189 0.590978 0.147745 12 /root/.icevision/data/fridge/odFridgeObjects/images/59.jpg 0.257267 0.262500 0.021031 0.022185 0.562983 0.140746 13 /root/.icevision/data/fridge/odFridgeObjects/images/90.jpg 0.260042 0.265823 0.004306 0.016405 0.546575 0.136644 14 /root/.icevision/data/fridge/odFridgeObjects/images/109.jpg 0.248338 0.260444 0.018180 0.017427 0.544389 0.136097 15 /root/.icevision/data/fridge/odFridgeObjects/images/16.jpg 0.221121 0.255459 0.002065 0.005470 0.484115 0.121029 16 /root/.icevision/data/fridge/odFridgeObjects/images/28.jpg 0.228037 0.208443 0.013820 0.018261 0.468562 0.117141 17 /root/.icevision/data/fridge/odFridgeObjects/images/80.jpg 0.218763 0.217823 0.007633 0.017448 0.461667 0.115417 18 /root/.icevision/data/fridge/odFridgeObjects/images/33.jpg 0.211880 0.199396 0.010208 0.016371 0.437855 0.109464 19 /root/.icevision/data/fridge/odFridgeObjects/images/123.jpg 0.181498 0.219479 0.001994 0.012886 0.415857 0.103964 20 /root/.icevision/data/fridge/odFridgeObjects/images/100.jpg 0.121183 0.203545 0.000172 0.001678 0.326578 0.081645 21 /root/.icevision/data/fridge/odFridgeObjects/images/102.jpg 0.141722 0.161956 0.000054 0.002683 0.306415 0.076604 22 /root/.icevision/data/fridge/odFridgeObjects/images/19.jpg 0.129916 0.137039 0.000416 0.007992 0.275363 0.068841 23 /root/.icevision/data/fridge/odFridgeObjects/images/3.jpg 0.134473 0.104869 0.005156 0.008392 0.252891 0.063223 24 /root/.icevision/data/fridge/odFridgeObjects/images/4.jpg 0.113395 0.092752 0.000150 0.003340 0.209638 0.052409 25 /root/.icevision/data/fridge/odFridgeObjects/images/1.jpg 0.080723 0.051906 0.001517 0.001408 0.135554 0.033889","title":"Run top_plot_losses on faster_rcnn model results"},{"location":"plot_top_losses/#run-top_plot_losses-on-a-retinanet-pretrained-but-not-finetuned-model","text":"model = retinanet . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=136595076.0), HTML(value=''))) by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_classification\" : 0.25 , \"loss_bbox_regression\" : 0.75 , } } sorted_samples , sorted_preds , losses_stats = retinanet . interp . plot_top_losses ( model , valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classification', 'loss_bbox_regression']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))","title":"Run top_plot_losses on a retinanet pretrained (but not finetuned) model"},{"location":"plot_top_losses/#run-top_plot_losses-on-a-efficientdet-pretrained-but-not-finetuned-model","text":"model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = 384 ) sorted_samples , sorted_preds , losses_stats = efficientdet . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))","title":"Run top_plot_losses on a efficientdet pretrained (but not finetuned) model"},{"location":"plot_top_losses/#instance-segmentation","text":"","title":"Instance Segmentation"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-mask_rcnn-model-on-the-pennfudan-dataset","text":"data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=53723336.0), HTML(value=''))) parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 1 , shuffle = True , num_workers = 0 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 1 , shuffle = False , num_workers = 0 ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m model = mask_rcnn . model ( num_classes = len ( class_map )) learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178090079.0), HTML(value=''))) epoch train_loss valid_loss time 0 0.702523 0.555716 00:18 1 0.657636 0.522894 00:14 epoch train_loss valid_loss time 0 0.551434 0.377621 00:20 1 0.469830 0.448473 00:19 2 0.472173 0.364438 00:19 3 0.437653 0.338397 00:19 4 0.375957 0.342569 00:19 5 0.385508 0.317213 00:20 6 0.371840 0.343802 00:19 7 0.356101 0.358258 00:19 8 0.484352 0.322044 00:19 9 0.339008 0.312049 00:19 sorted_samples , sorted_preds , losses_stats = mask_rcnn . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=34.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))","title":"plot_top_losses in action with a mask_rcnn model on the pennfudan dataset"},{"location":"plot_top_losses/#keypoint-detection","text":"","title":"Keypoint Detection"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-keypoint_rcnn-model-on-the-biwi-dataset","text":"data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = keypoint_rcnn . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = keypoint_rcnn . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=593774.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m126\u001b[0m backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = keypoint_rcnn . model ( backbone = backbone , num_keypoints = 1 ) Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value=''))) learn = keypoint_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss time 0 9.269516 8.537302 00:35 1 8.527759 7.503790 00:16 epoch train_loss valid_loss time 0 7.033783 6.651293 00:22 1 6.630652 5.782725 00:24 2 6.220629 5.254831 00:15 3 5.921734 5.051409 00:19 4 5.713860 4.953146 00:17 sorted_samples , sorted_preds , losses_stats = keypoint_rcnn . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m206\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=40.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))","title":"plot_top_losses in action with a keypoint_rcnn model on the biwi dataset"},{"location":"quickstart/","text":"Quickstart EfficientDet: Scalable and Efficient Object Detection Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata. Installing IceVision and IceData ! pip install icevision [ all ] icedata Imports from icevision.all import * Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = 384 ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.108684 1.270576 0.008988 00:06 1 1.756881 1.273853 0.006717 00:05 2 1.595295 1.305949 0.009512 00:05 3 1.441863 1.150711 0.145460 00:05 4 1.313717 1.087273 0.142000 00:05 epoch train_loss valid_loss COCOMetric time 0 0.796611 0.987059 0.199351 00:07 1 0.764935 0.936920 0.196103 00:06 2 0.737171 0.865462 0.250470 00:06 3 0.707076 0.830861 0.291164 00:06 4 0.679944 0.785556 0.289883 00:06 5 0.657874 0.705734 0.371499 00:06 6 0.630947 0.657513 0.440564 00:06 7 0.608779 0.629073 0.462677 00:05 8 0.594612 0.531862 0.504480 00:06 9 0.573904 0.478792 0.548560 00:06 10 0.553602 0.436366 0.682166 00:05 11 0.526821 0.431016 0.669347 00:05 12 0.503331 0.443858 0.601617 00:05 13 0.490109 0.452481 0.594377 00:06 14 0.470531 0.462492 0.670671 00:05 15 0.455318 0.377674 0.681230 00:05 16 0.441404 0.409097 0.678400 00:05 17 0.426164 0.358776 0.697379 00:06 18 0.413478 0.395500 0.636250 00:06 19 0.404278 0.367352 0.668839 00:06 20 0.387968 0.390063 0.663779 00:06 21 0.374634 0.284140 0.730051 00:06 22 0.366777 0.280615 0.741545 00:06 23 0.359820 0.301929 0.686845 00:06 24 0.347181 0.300537 0.730203 00:06 25 0.338814 0.269294 0.767164 00:06 26 0.326413 0.245472 0.788555 00:06 27 0.314856 0.253438 0.784647 00:06 28 0.306478 0.227623 0.806798 00:06 29 0.297792 0.273537 0.726833 00:06 30 0.291198 0.215873 0.821891 00:06 31 0.283369 0.217524 0.827257 00:06 32 0.277785 0.213709 0.831023 00:05 33 0.273086 0.208569 0.816660 00:05 34 0.264948 0.216965 0.819404 00:06 35 0.257523 0.187829 0.844655 00:06 36 0.255689 0.197039 0.843100 00:06 37 0.250191 0.209165 0.777218 00:06 38 0.245002 0.181346 0.831535 00:05 39 0.241197 0.187171 0.822547 00:05 40 0.239846 0.177838 0.833223 00:05 41 0.234420 0.175321 0.827860 00:06 42 0.230797 0.168390 0.857568 00:05 43 0.227416 0.167590 0.858564 00:06 44 0.225182 0.167128 0.874714 00:05 45 0.223807 0.165946 0.870924 00:05 46 0.220755 0.164731 0.872577 00:05 47 0.216147 0.164051 0.869572 00:06 48 0.214723 0.162329 0.870547 00:05 49 0.212741 0.162104 0.869900 00:05 Training using Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 50 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model , infer_dl ) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Quickstart using EfficientDet"},{"location":"quickstart/#quickstart","text":"EfficientDet: Scalable and Efficient Object Detection","title":"Quickstart"},{"location":"quickstart/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata.","title":"Introduction"},{"location":"quickstart/#installing-icevision-and-icedata","text":"! pip install icevision [ all ] icedata","title":"Installing IceVision and IceData"},{"location":"quickstart/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"quickstart/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse ()","title":"Datasets : Fridge Objects dataset"},{"location":"quickstart/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"quickstart/#train-and-validation-dataset-transforms","text":"# Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"quickstart/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"quickstart/#dataloader","text":"# DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"quickstart/#model","text":"model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = 384 )","title":"Model"},{"location":"quickstart/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"quickstart/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"quickstart/#training-using-fastai","text":"learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.108684 1.270576 0.008988 00:06 1 1.756881 1.273853 0.006717 00:05 2 1.595295 1.305949 0.009512 00:05 3 1.441863 1.150711 0.145460 00:05 4 1.313717 1.087273 0.142000 00:05 epoch train_loss valid_loss COCOMetric time 0 0.796611 0.987059 0.199351 00:07 1 0.764935 0.936920 0.196103 00:06 2 0.737171 0.865462 0.250470 00:06 3 0.707076 0.830861 0.291164 00:06 4 0.679944 0.785556 0.289883 00:06 5 0.657874 0.705734 0.371499 00:06 6 0.630947 0.657513 0.440564 00:06 7 0.608779 0.629073 0.462677 00:05 8 0.594612 0.531862 0.504480 00:06 9 0.573904 0.478792 0.548560 00:06 10 0.553602 0.436366 0.682166 00:05 11 0.526821 0.431016 0.669347 00:05 12 0.503331 0.443858 0.601617 00:05 13 0.490109 0.452481 0.594377 00:06 14 0.470531 0.462492 0.670671 00:05 15 0.455318 0.377674 0.681230 00:05 16 0.441404 0.409097 0.678400 00:05 17 0.426164 0.358776 0.697379 00:06 18 0.413478 0.395500 0.636250 00:06 19 0.404278 0.367352 0.668839 00:06 20 0.387968 0.390063 0.663779 00:06 21 0.374634 0.284140 0.730051 00:06 22 0.366777 0.280615 0.741545 00:06 23 0.359820 0.301929 0.686845 00:06 24 0.347181 0.300537 0.730203 00:06 25 0.338814 0.269294 0.767164 00:06 26 0.326413 0.245472 0.788555 00:06 27 0.314856 0.253438 0.784647 00:06 28 0.306478 0.227623 0.806798 00:06 29 0.297792 0.273537 0.726833 00:06 30 0.291198 0.215873 0.821891 00:06 31 0.283369 0.217524 0.827257 00:06 32 0.277785 0.213709 0.831023 00:05 33 0.273086 0.208569 0.816660 00:05 34 0.264948 0.216965 0.819404 00:06 35 0.257523 0.187829 0.844655 00:06 36 0.255689 0.197039 0.843100 00:06 37 0.250191 0.209165 0.777218 00:06 38 0.245002 0.181346 0.831535 00:05 39 0.241197 0.187171 0.822547 00:05 40 0.239846 0.177838 0.833223 00:05 41 0.234420 0.175321 0.827860 00:06 42 0.230797 0.168390 0.857568 00:05 43 0.227416 0.167590 0.858564 00:06 44 0.225182 0.167128 0.874714 00:05 45 0.223807 0.165946 0.870924 00:05 46 0.220755 0.164731 0.872577 00:05 47 0.216147 0.164051 0.869572 00:06 48 0.214723 0.162329 0.870547 00:05 49 0.212741 0.162104 0.869900 00:05","title":"Training using fastai"},{"location":"quickstart/#training-using-lightning","text":"class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 50 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"quickstart/#inference","text":"","title":"Inference"},{"location":"quickstart/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model , infer_dl ) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Predicting a batch of images"},{"location":"quickstart/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' )","title":"Saving Model on Google Drive"},{"location":"quickstart/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"quickstart_ResNeSt/","text":"Quickstart Using ResNeSt Paper: ResNeSt: Split-Attention Networks Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata. Installing IceVision and IceData # !pip install icevision[all] icedata pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade Imports from icevision.all import * from fastai.callback.tracker import SaveModelCallback Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader model_type = faster_rcnn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) Model # Get a pretrained ResNeSt FPN Backbone: e.g restnest50 # Existing pretrained resnest backbones are: resnest50, resnest101, resnest200, resnest269 backbone = backbones . resnest_fpn . resnest50 ( pretrained = True ) Downloading: \"https://s3.us-west-1.wasabisys.com/resnest/torch/resnest50-528c19ca.pth\" to /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=110273258.0), HTML(value=''))) model = model_type . model ( backbone = backbone , num_classes = len ( class_map )) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.019054606556892395) learn . fine_tune ( 50 , 2e-4 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.762670 1.127936 0.000555 00:07 1 1.327857 0.991028 0.000120 00:06 2 1.174690 1.015997 0.000048 00:06 3 1.089509 0.957638 0.011715 00:06 4 1.031818 0.890940 0.021980 00:06 epoch train_loss valid_loss COCOMetric time 0 0.872708 0.856233 0.066737 00:08 1 0.910392 0.856490 0.095219 00:07 2 0.923059 0.807127 0.102635 00:07 3 0.893881 0.706737 0.181068 00:07 4 0.846220 0.577118 0.206684 00:07 5 0.770894 0.443069 0.226011 00:07 6 0.690153 0.358363 0.231685 00:07 7 0.614847 0.324193 0.306921 00:07 8 0.547570 0.295342 0.349507 00:07 9 0.499190 0.346569 0.388942 00:07 10 0.463566 0.377226 0.395301 00:07 11 0.440261 0.380914 0.429039 00:07 12 0.419555 0.347445 0.490683 00:07 13 0.408768 0.374569 0.435365 00:07 14 0.395535 0.372383 0.535321 00:07 15 0.376081 0.366716 0.542196 00:07 16 0.362154 0.389555 0.492635 00:07 17 0.346421 0.313668 0.538568 00:07 18 0.334581 0.362932 0.547420 00:07 19 0.322343 0.314747 0.618869 00:07 20 0.306562 0.406998 0.461870 00:07 21 0.295246 0.312923 0.612174 00:07 22 0.285954 0.340467 0.580474 00:07 23 0.274488 0.283455 0.684320 00:07 24 0.268342 0.324935 0.651953 00:07 25 0.262872 0.286207 0.641081 00:07 26 0.252152 0.253029 0.686516 00:07 27 0.248780 0.328883 0.609743 00:07 28 0.239899 0.320969 0.628861 00:07 29 0.234647 0.251311 0.706407 00:07 30 0.227755 0.256202 0.720632 00:07 31 0.221247 0.255702 0.704574 00:07 32 0.214771 0.235117 0.731469 00:07 33 0.209753 0.241401 0.734679 00:07 34 0.206958 0.263137 0.748318 00:07 35 0.202806 0.249361 0.741839 00:07 36 0.201422 0.254605 0.759009 00:07 37 0.195052 0.248898 0.761324 00:07 38 0.189870 0.235543 0.734376 00:07 39 0.187547 0.244468 0.743984 00:07 40 0.185551 0.243460 0.757906 00:07 41 0.183169 0.240184 0.757863 00:07 42 0.177280 0.219689 0.748292 00:07 43 0.174024 0.235657 0.770797 00:07 44 0.173908 0.237733 0.762268 00:07 45 0.171298 0.236662 0.750911 00:07 46 0.170103 0.237634 0.744570 00:07 47 0.169592 0.237385 0.764241 00:07 48 0.170311 0.230228 0.751326 00:07 49 0.173960 0.239171 0.772328 00:07 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = model_type . predict_dl ( model , infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_restnest_fpn50.pth' ) Mounted at /content/gdrive Happy Learning! If you need any assistance, feel free to join our forum .","title":"quickstart ResNeSt"},{"location":"quickstart_ResNeSt/#quickstart-using-resnest","text":"Paper: ResNeSt: Split-Attention Networks","title":"Quickstart Using ResNeSt"},{"location":"quickstart_ResNeSt/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata.","title":"Introduction"},{"location":"quickstart_ResNeSt/#installing-icevision-and-icedata","text":"# !pip install icevision[all] icedata pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade","title":"Installing IceVision and IceData"},{"location":"quickstart_ResNeSt/#imports","text":"from icevision.all import * from fastai.callback.tracker import SaveModelCallback","title":"Imports"},{"location":"quickstart_ResNeSt/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m","title":"Datasets : Fridge Objects dataset"},{"location":"quickstart_ResNeSt/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"quickstart_ResNeSt/#train-and-validation-dataset-transforms","text":"# Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"quickstart_ResNeSt/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"quickstart_ResNeSt/#dataloader","text":"model_type = faster_rcnn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"quickstart_ResNeSt/#model","text":"# Get a pretrained ResNeSt FPN Backbone: e.g restnest50 # Existing pretrained resnest backbones are: resnest50, resnest101, resnest200, resnest269 backbone = backbones . resnest_fpn . resnest50 ( pretrained = True ) Downloading: \"https://s3.us-west-1.wasabisys.com/resnest/torch/resnest50-528c19ca.pth\" to /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=110273258.0), HTML(value=''))) model = model_type . model ( backbone = backbone , num_classes = len ( class_map ))","title":"Model"},{"location":"quickstart_ResNeSt/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"quickstart_ResNeSt/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"quickstart_ResNeSt/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.019054606556892395) learn . fine_tune ( 50 , 2e-4 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.762670 1.127936 0.000555 00:07 1 1.327857 0.991028 0.000120 00:06 2 1.174690 1.015997 0.000048 00:06 3 1.089509 0.957638 0.011715 00:06 4 1.031818 0.890940 0.021980 00:06 epoch train_loss valid_loss COCOMetric time 0 0.872708 0.856233 0.066737 00:08 1 0.910392 0.856490 0.095219 00:07 2 0.923059 0.807127 0.102635 00:07 3 0.893881 0.706737 0.181068 00:07 4 0.846220 0.577118 0.206684 00:07 5 0.770894 0.443069 0.226011 00:07 6 0.690153 0.358363 0.231685 00:07 7 0.614847 0.324193 0.306921 00:07 8 0.547570 0.295342 0.349507 00:07 9 0.499190 0.346569 0.388942 00:07 10 0.463566 0.377226 0.395301 00:07 11 0.440261 0.380914 0.429039 00:07 12 0.419555 0.347445 0.490683 00:07 13 0.408768 0.374569 0.435365 00:07 14 0.395535 0.372383 0.535321 00:07 15 0.376081 0.366716 0.542196 00:07 16 0.362154 0.389555 0.492635 00:07 17 0.346421 0.313668 0.538568 00:07 18 0.334581 0.362932 0.547420 00:07 19 0.322343 0.314747 0.618869 00:07 20 0.306562 0.406998 0.461870 00:07 21 0.295246 0.312923 0.612174 00:07 22 0.285954 0.340467 0.580474 00:07 23 0.274488 0.283455 0.684320 00:07 24 0.268342 0.324935 0.651953 00:07 25 0.262872 0.286207 0.641081 00:07 26 0.252152 0.253029 0.686516 00:07 27 0.248780 0.328883 0.609743 00:07 28 0.239899 0.320969 0.628861 00:07 29 0.234647 0.251311 0.706407 00:07 30 0.227755 0.256202 0.720632 00:07 31 0.221247 0.255702 0.704574 00:07 32 0.214771 0.235117 0.731469 00:07 33 0.209753 0.241401 0.734679 00:07 34 0.206958 0.263137 0.748318 00:07 35 0.202806 0.249361 0.741839 00:07 36 0.201422 0.254605 0.759009 00:07 37 0.195052 0.248898 0.761324 00:07 38 0.189870 0.235543 0.734376 00:07 39 0.187547 0.244468 0.743984 00:07 40 0.185551 0.243460 0.757906 00:07 41 0.183169 0.240184 0.757863 00:07 42 0.177280 0.219689 0.748292 00:07 43 0.174024 0.235657 0.770797 00:07 44 0.173908 0.237733 0.762268 00:07 45 0.171298 0.236662 0.750911 00:07 46 0.170103 0.237634 0.744570 00:07 47 0.169592 0.237385 0.764241 00:07 48 0.170311 0.230228 0.751326 00:07 49 0.173960 0.239171 0.772328 00:07","title":"Training using fastai"},{"location":"quickstart_ResNeSt/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"quickstart_ResNeSt/#inference","text":"","title":"Inference"},{"location":"quickstart_ResNeSt/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = model_type . predict_dl ( model , infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Predicting a batch of images"},{"location":"quickstart_ResNeSt/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_restnest_fpn50.pth' ) Mounted at /content/gdrive","title":"Saving Model on Google Drive"},{"location":"quickstart_ResNeSt/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"quickstart_RestNeSt/","text":"Quickstart Using ResNeSt Paper: ResNeSt: Split-Attention Networks We added support of the ResNeSt backbones. You can read more about that model here: https://discordapp.com/channels/735877944085446747/735877944517591151/780303631738994700 We created a RestNeSt FPN backbone from the published ResNest model (by adding this soft dependency: resnest ). We support all the pretrained resnest backbones: resnest50, resnest101, resnest200, resnest269. Since we provide RestNeSt FPN backbones, we can use them in all the FPN compatible models such as Faster-RCNN, Mask-RCNN, and RetinaNet. Our RestNeSt FPN API is very similar to the RestNet FPN: With ResNet: backbone = backbones.resnet_fpn.resnet50(pretrained=True) With ResNeSt: (Add just an \"s\" to resnet -> resnest) backbone = backbones.resnest_fpn.resnest50(pretrained=True) And this is the common part for both ResNet and ResNeSt: model_type = faster_rcnn # faster_rcnn mask_rcnn retinanet model = model_type.model(backbone = backbone, num_classes=len(class_map)) Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata. Installing IceVision and IceData # !pip install icevision[all] icedata ! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade Imports from icevision.all import * from fastai.callback.tracker import SaveModelCallback Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader model_type = faster_rcnn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) ResNeSt Backbone # Get a pretrained ResNeSt FPN Backbone: e.g restnest50 # Existing pretrained resnest backbones are: resnest50, resnest101, resnest200, resnest269 backbone = backbones . resnest_fpn . resnest50 ( pretrained = True ) Downloading: \"https://s3.us-west-1.wasabisys.com/resnest/torch/resnest50-528c19ca.pth\" to /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=110273258.0), HTML(value=''))) Model model = model_type . model ( backbone = backbone , num_classes = len ( class_map )) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.019054606556892395) learn . fine_tune ( 50 , 2e-4 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.762670 1.127936 0.000555 00:07 1 1.327857 0.991028 0.000120 00:06 2 1.174690 1.015997 0.000048 00:06 3 1.089509 0.957638 0.011715 00:06 4 1.031818 0.890940 0.021980 00:06 epoch train_loss valid_loss COCOMetric time 0 0.872708 0.856233 0.066737 00:08 1 0.910392 0.856490 0.095219 00:07 2 0.923059 0.807127 0.102635 00:07 3 0.893881 0.706737 0.181068 00:07 4 0.846220 0.577118 0.206684 00:07 5 0.770894 0.443069 0.226011 00:07 6 0.690153 0.358363 0.231685 00:07 7 0.614847 0.324193 0.306921 00:07 8 0.547570 0.295342 0.349507 00:07 9 0.499190 0.346569 0.388942 00:07 10 0.463566 0.377226 0.395301 00:07 11 0.440261 0.380914 0.429039 00:07 12 0.419555 0.347445 0.490683 00:07 13 0.408768 0.374569 0.435365 00:07 14 0.395535 0.372383 0.535321 00:07 15 0.376081 0.366716 0.542196 00:07 16 0.362154 0.389555 0.492635 00:07 17 0.346421 0.313668 0.538568 00:07 18 0.334581 0.362932 0.547420 00:07 19 0.322343 0.314747 0.618869 00:07 20 0.306562 0.406998 0.461870 00:07 21 0.295246 0.312923 0.612174 00:07 22 0.285954 0.340467 0.580474 00:07 23 0.274488 0.283455 0.684320 00:07 24 0.268342 0.324935 0.651953 00:07 25 0.262872 0.286207 0.641081 00:07 26 0.252152 0.253029 0.686516 00:07 27 0.248780 0.328883 0.609743 00:07 28 0.239899 0.320969 0.628861 00:07 29 0.234647 0.251311 0.706407 00:07 30 0.227755 0.256202 0.720632 00:07 31 0.221247 0.255702 0.704574 00:07 32 0.214771 0.235117 0.731469 00:07 33 0.209753 0.241401 0.734679 00:07 34 0.206958 0.263137 0.748318 00:07 35 0.202806 0.249361 0.741839 00:07 36 0.201422 0.254605 0.759009 00:07 37 0.195052 0.248898 0.761324 00:07 38 0.189870 0.235543 0.734376 00:07 39 0.187547 0.244468 0.743984 00:07 40 0.185551 0.243460 0.757906 00:07 41 0.183169 0.240184 0.757863 00:07 42 0.177280 0.219689 0.748292 00:07 43 0.174024 0.235657 0.770797 00:07 44 0.173908 0.237733 0.762268 00:07 45 0.171298 0.236662 0.750911 00:07 46 0.170103 0.237634 0.744570 00:07 47 0.169592 0.237385 0.764241 00:07 48 0.170311 0.230228 0.751326 00:07 49 0.173960 0.239171 0.772328 00:07 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = model_type . predict_dl ( model , infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_restnest_fpn50.pth' ) Mounted at /content/gdrive Happy Learning! If you need any assistance, feel free to join our forum .","title":"quickstart RestNeSt"},{"location":"quickstart_RestNeSt/#quickstart-using-resnest","text":"Paper: ResNeSt: Split-Attention Networks We added support of the ResNeSt backbones. You can read more about that model here: https://discordapp.com/channels/735877944085446747/735877944517591151/780303631738994700 We created a RestNeSt FPN backbone from the published ResNest model (by adding this soft dependency: resnest ). We support all the pretrained resnest backbones: resnest50, resnest101, resnest200, resnest269. Since we provide RestNeSt FPN backbones, we can use them in all the FPN compatible models such as Faster-RCNN, Mask-RCNN, and RetinaNet. Our RestNeSt FPN API is very similar to the RestNet FPN: With ResNet: backbone = backbones.resnet_fpn.resnet50(pretrained=True) With ResNeSt: (Add just an \"s\" to resnet -> resnest) backbone = backbones.resnest_fpn.resnest50(pretrained=True) And this is the common part for both ResNet and ResNeSt: model_type = faster_rcnn # faster_rcnn mask_rcnn retinanet model = model_type.model(backbone = backbone, num_classes=len(class_map))","title":"Quickstart Using ResNeSt"},{"location":"quickstart_RestNeSt/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata.","title":"Introduction"},{"location":"quickstart_RestNeSt/#installing-icevision-and-icedata","text":"# !pip install icevision[all] icedata ! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade","title":"Installing IceVision and IceData"},{"location":"quickstart_RestNeSt/#imports","text":"from icevision.all import * from fastai.callback.tracker import SaveModelCallback","title":"Imports"},{"location":"quickstart_RestNeSt/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m","title":"Datasets : Fridge Objects dataset"},{"location":"quickstart_RestNeSt/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"quickstart_RestNeSt/#train-and-validation-dataset-transforms","text":"# Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"quickstart_RestNeSt/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"quickstart_RestNeSt/#dataloader","text":"model_type = faster_rcnn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"quickstart_RestNeSt/#resnest-backbone","text":"# Get a pretrained ResNeSt FPN Backbone: e.g restnest50 # Existing pretrained resnest backbones are: resnest50, resnest101, resnest200, resnest269 backbone = backbones . resnest_fpn . resnest50 ( pretrained = True ) Downloading: \"https://s3.us-west-1.wasabisys.com/resnest/torch/resnest50-528c19ca.pth\" to /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=110273258.0), HTML(value='')))","title":"ResNeSt Backbone"},{"location":"quickstart_RestNeSt/#model","text":"model = model_type . model ( backbone = backbone , num_classes = len ( class_map ))","title":"Model"},{"location":"quickstart_RestNeSt/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"quickstart_RestNeSt/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"quickstart_RestNeSt/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.019054606556892395) learn . fine_tune ( 50 , 2e-4 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.762670 1.127936 0.000555 00:07 1 1.327857 0.991028 0.000120 00:06 2 1.174690 1.015997 0.000048 00:06 3 1.089509 0.957638 0.011715 00:06 4 1.031818 0.890940 0.021980 00:06 epoch train_loss valid_loss COCOMetric time 0 0.872708 0.856233 0.066737 00:08 1 0.910392 0.856490 0.095219 00:07 2 0.923059 0.807127 0.102635 00:07 3 0.893881 0.706737 0.181068 00:07 4 0.846220 0.577118 0.206684 00:07 5 0.770894 0.443069 0.226011 00:07 6 0.690153 0.358363 0.231685 00:07 7 0.614847 0.324193 0.306921 00:07 8 0.547570 0.295342 0.349507 00:07 9 0.499190 0.346569 0.388942 00:07 10 0.463566 0.377226 0.395301 00:07 11 0.440261 0.380914 0.429039 00:07 12 0.419555 0.347445 0.490683 00:07 13 0.408768 0.374569 0.435365 00:07 14 0.395535 0.372383 0.535321 00:07 15 0.376081 0.366716 0.542196 00:07 16 0.362154 0.389555 0.492635 00:07 17 0.346421 0.313668 0.538568 00:07 18 0.334581 0.362932 0.547420 00:07 19 0.322343 0.314747 0.618869 00:07 20 0.306562 0.406998 0.461870 00:07 21 0.295246 0.312923 0.612174 00:07 22 0.285954 0.340467 0.580474 00:07 23 0.274488 0.283455 0.684320 00:07 24 0.268342 0.324935 0.651953 00:07 25 0.262872 0.286207 0.641081 00:07 26 0.252152 0.253029 0.686516 00:07 27 0.248780 0.328883 0.609743 00:07 28 0.239899 0.320969 0.628861 00:07 29 0.234647 0.251311 0.706407 00:07 30 0.227755 0.256202 0.720632 00:07 31 0.221247 0.255702 0.704574 00:07 32 0.214771 0.235117 0.731469 00:07 33 0.209753 0.241401 0.734679 00:07 34 0.206958 0.263137 0.748318 00:07 35 0.202806 0.249361 0.741839 00:07 36 0.201422 0.254605 0.759009 00:07 37 0.195052 0.248898 0.761324 00:07 38 0.189870 0.235543 0.734376 00:07 39 0.187547 0.244468 0.743984 00:07 40 0.185551 0.243460 0.757906 00:07 41 0.183169 0.240184 0.757863 00:07 42 0.177280 0.219689 0.748292 00:07 43 0.174024 0.235657 0.770797 00:07 44 0.173908 0.237733 0.762268 00:07 45 0.171298 0.236662 0.750911 00:07 46 0.170103 0.237634 0.744570 00:07 47 0.169592 0.237385 0.764241 00:07 48 0.170311 0.230228 0.751326 00:07 49 0.173960 0.239171 0.772328 00:07","title":"Training using fastai"},{"location":"quickstart_RestNeSt/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"quickstart_RestNeSt/#inference","text":"","title":"Inference"},{"location":"quickstart_RestNeSt/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = model_type . predict_dl ( model , infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value=''))) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Predicting a batch of images"},{"location":"quickstart_RestNeSt/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_restnest_fpn50.pth' ) Mounted at /content/gdrive","title":"Saving Model on Google Drive"},{"location":"quickstart_RestNeSt/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"readme_mkdocs/","text":"IceVision Documentation The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Generating Docs"},{"location":"readme_mkdocs/#icevision-documentation","text":"The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"IceVision Documentation"},{"location":"readme_mkdocs/#building-the-documentation","text":"Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Building the documentation"},{"location":"retinanet/","text":"Quickstart Using RetinaNet RetinaNet: Focal Loss for Dense Object Detection Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata. Installing IceVision and IceData # !pip install icevision[all] icedata ! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade Imports from icevision.all import * Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader # DataLoaders train_dl = retinanet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = retinanet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) Model model = retinanet . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=136595076.0), HTML(value=''))) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = retinanet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) SuggestedLRs(lr_min=2.7542287716642023e-05, lr_steep=0.0003311311302240938) learn . fine_tune ( 50 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.773187 1.721653 0.000000 00:06 /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) epoch train_loss valid_loss COCOMetric time 0 1.669240 1.604600 0.000000 00:07 1 1.511697 1.204714 0.013421 00:07 2 1.351895 1.066002 0.013898 00:07 3 1.231514 0.937882 0.017117 00:07 4 1.140697 0.878017 0.038607 00:07 5 1.070421 0.831975 0.037769 00:07 6 1.009788 0.793019 0.111913 00:07 7 0.960639 0.728101 0.119228 00:07 8 0.912715 0.671543 0.146560 00:07 9 0.870147 0.626667 0.171220 00:07 10 0.832594 0.599739 0.203036 00:07 11 0.797179 0.589813 0.218498 00:07 12 0.762873 0.552396 0.239216 00:06 13 0.736472 0.571118 0.228611 00:06 14 0.708510 0.524506 0.298357 00:06 15 0.684319 0.510075 0.288280 00:06 16 0.660906 0.455961 0.443214 00:06 17 0.635326 0.470874 0.428143 00:06 18 0.617463 0.439645 0.461565 00:06 19 0.596102 0.432902 0.466235 00:06 20 0.573141 0.391897 0.503541 00:06 21 0.554290 0.395687 0.559103 00:06 22 0.537533 0.374369 0.616710 00:06 23 0.521039 0.353456 0.582087 00:06 24 0.500212 0.365042 0.588221 00:06 25 0.481130 0.380503 0.578897 00:06 26 0.461111 0.311713 0.671479 00:06 27 0.443895 0.273624 0.755427 00:06 28 0.426764 0.292557 0.726729 00:06 29 0.410741 0.273373 0.743065 00:06 30 0.395672 0.263620 0.731577 00:06 31 0.381944 0.254182 0.779902 00:06 32 0.364849 0.255048 0.778393 00:06 33 0.353671 0.262039 0.758704 00:06 34 0.343300 0.250880 0.787915 00:06 35 0.332495 0.230082 0.807588 00:06 36 0.320344 0.219029 0.819196 00:06 37 0.311264 0.238515 0.781773 00:06 38 0.301364 0.209411 0.824190 00:06 39 0.291357 0.209720 0.831019 00:06 40 0.282660 0.206470 0.845421 00:06 41 0.274021 0.201741 0.845945 00:06 42 0.267398 0.198581 0.836714 00:06 43 0.259853 0.201253 0.836890 00:06 44 0.254091 0.200994 0.844222 00:06 45 0.247125 0.196282 0.852836 00:06 46 0.242366 0.195580 0.850415 00:06 47 0.237883 0.196337 0.842754 00:06 48 0.234541 0.196949 0.841875 00:06 49 0.232748 0.197014 0.841945 00:06 Training using Lightning class LightModel ( retinanet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 40 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = retinanet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = retinanet . predict_dl ( model , infer_dl ) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_fpn50.pth' ) Mounted at /content/gdrive Happy Learning! If you need any assistance, feel free to join our forum .","title":"Quickstart using RetinaNet"},{"location":"retinanet/#quickstart-using-retinanet","text":"RetinaNet: Focal Loss for Dense Object Detection","title":"Quickstart Using RetinaNet"},{"location":"retinanet/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata.","title":"Introduction"},{"location":"retinanet/#installing-icevision-and-icedata","text":"# !pip install icevision[all] icedata ! pip install git + git : // github . com / airctic / icevision . git #egg=icevision[all] icedata --upgrade","title":"Installing IceVision and IceData"},{"location":"retinanet/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"retinanet/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20380998.0), HTML(value=''))) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m","title":"Datasets : Fridge Objects dataset"},{"location":"retinanet/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"retinanet/#train-and-validation-dataset-transforms","text":"# Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"retinanet/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"retinanet/#dataloader","text":"# DataLoaders train_dl = retinanet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = retinanet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"retinanet/#model","text":"model = retinanet . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=136595076.0), HTML(value='')))","title":"Model"},{"location":"retinanet/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"retinanet/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"retinanet/#training-using-fastai","text":"learn = retinanet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) SuggestedLRs(lr_min=2.7542287716642023e-05, lr_steep=0.0003311311302240938) learn . fine_tune ( 50 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.773187 1.721653 0.000000 00:06 /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead. warnings.warn(warning.format(ret)) epoch train_loss valid_loss COCOMetric time 0 1.669240 1.604600 0.000000 00:07 1 1.511697 1.204714 0.013421 00:07 2 1.351895 1.066002 0.013898 00:07 3 1.231514 0.937882 0.017117 00:07 4 1.140697 0.878017 0.038607 00:07 5 1.070421 0.831975 0.037769 00:07 6 1.009788 0.793019 0.111913 00:07 7 0.960639 0.728101 0.119228 00:07 8 0.912715 0.671543 0.146560 00:07 9 0.870147 0.626667 0.171220 00:07 10 0.832594 0.599739 0.203036 00:07 11 0.797179 0.589813 0.218498 00:07 12 0.762873 0.552396 0.239216 00:06 13 0.736472 0.571118 0.228611 00:06 14 0.708510 0.524506 0.298357 00:06 15 0.684319 0.510075 0.288280 00:06 16 0.660906 0.455961 0.443214 00:06 17 0.635326 0.470874 0.428143 00:06 18 0.617463 0.439645 0.461565 00:06 19 0.596102 0.432902 0.466235 00:06 20 0.573141 0.391897 0.503541 00:06 21 0.554290 0.395687 0.559103 00:06 22 0.537533 0.374369 0.616710 00:06 23 0.521039 0.353456 0.582087 00:06 24 0.500212 0.365042 0.588221 00:06 25 0.481130 0.380503 0.578897 00:06 26 0.461111 0.311713 0.671479 00:06 27 0.443895 0.273624 0.755427 00:06 28 0.426764 0.292557 0.726729 00:06 29 0.410741 0.273373 0.743065 00:06 30 0.395672 0.263620 0.731577 00:06 31 0.381944 0.254182 0.779902 00:06 32 0.364849 0.255048 0.778393 00:06 33 0.353671 0.262039 0.758704 00:06 34 0.343300 0.250880 0.787915 00:06 35 0.332495 0.230082 0.807588 00:06 36 0.320344 0.219029 0.819196 00:06 37 0.311264 0.238515 0.781773 00:06 38 0.301364 0.209411 0.824190 00:06 39 0.291357 0.209720 0.831019 00:06 40 0.282660 0.206470 0.845421 00:06 41 0.274021 0.201741 0.845945 00:06 42 0.267398 0.198581 0.836714 00:06 43 0.259853 0.201253 0.836890 00:06 44 0.254091 0.200994 0.844222 00:06 45 0.247125 0.196282 0.852836 00:06 46 0.242366 0.195580 0.850415 00:06 47 0.237883 0.196337 0.842754 00:06 48 0.234541 0.196949 0.841875 00:06 49 0.232748 0.197014 0.841945 00:06","title":"Training using fastai"},{"location":"retinanet/#training-using-lightning","text":"class LightModel ( retinanet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 40 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"retinanet/#inference","text":"","title":"Inference"},{"location":"retinanet/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = retinanet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = retinanet . predict_dl ( model , infer_dl ) show_preds ( samples = samples [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Predicting a batch of images"},{"location":"retinanet/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_fpn50.pth' ) Mounted at /content/gdrive","title":"Saving Model on Google Drive"},{"location":"retinanet/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"voc_predefined_splits/","text":"How to parse a voc dataset using predefined splits Install and import IceVision and IceData ! pip install git + git : // github . com / airctic / icevision . git from icevision.all import * Load Pascal VOC 2012 dataset path = icedata . voc . load_data () Set images, annotations and imagesets directories annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\" Define class_map class_map = icedata . voc . class_map () Split data using imagesets ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by imageid in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits ) Parser: use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) Train and validation records train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 2 ], ncols = 2 , class_map = class_map )","title":"Fixed Splitter"},{"location":"voc_predefined_splits/#how-to-parse-a-voc-dataset-using-predefined-splits","text":"","title":"How to parse a voc dataset using predefined splits"},{"location":"voc_predefined_splits/#install-and-import-icevision-and-icedata","text":"! pip install git + git : // github . com / airctic / icevision . git from icevision.all import *","title":"Install and import IceVision and IceData"},{"location":"voc_predefined_splits/#load-pascal-voc-2012-dataset","text":"path = icedata . voc . load_data ()","title":"Load Pascal VOC 2012 dataset"},{"location":"voc_predefined_splits/#set-images-annotations-and-imagesets-directories","text":"annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\"","title":"Set images, annotations and imagesets directories"},{"location":"voc_predefined_splits/#define-class_map","text":"class_map = icedata . voc . class_map ()","title":"Define class_map"},{"location":"voc_predefined_splits/#split-data-using-imagesets","text":"ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by imageid in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits )","title":"Split data using imagesets"},{"location":"voc_predefined_splits/#parser-use-icevision-predefined-voc-parser","text":"parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map )","title":"Parser: use icevision predefined VOC parser"},{"location":"voc_predefined_splits/#train-and-validation-records","text":"train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 2 ], ncols = 2 , class_map = class_map )","title":"Train and validation records"},{"location":"wandb_efficientdet/","text":"IceVision meets W&B IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking For more information check the following Report IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. In this example, we are using the fastai training loop, it offers a slick integration with wandb through the use of the WandbCallback() callback. Introduction In this tutorial, we walk you through the different steps of training the fridge dataset. Thanks to W&B, we can easily track the performance of the EfficientDet model using different backbones. In this example, we are using the fastai library training loop. Installing IceVision and IceData ! pip install icevision [ all ] icedata Imports from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) Model # EfficientDet D2 model = efficientdet . model ( 'tf_efficientdet_d2' , num_classes = len ( class_map ), img_size = 384 ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_d2-1\" , reinit = True ) <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc Tracking run with wandb version 0.10.8 Syncing run efficientdet_d2-1 to Weights & Biases (Documentation) . Project page: https://wandb.ai/ai-fast-track/icevision-fridge Run page: https://wandb.ai/ai-fast-track/icevision-fridge/runs/m4mz59fz Run data is saved locally in wandb/run-20201027_134038-m4mz59fz Run(m4mz59fz) learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics , cbs = [ WandbCallback (), SaveModelCallback ()]) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 5 ) Show results efficientdet . show_results ( model , valid_ds , class_map = class_map ) Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model = model , infer_dl = infer_dl ) # from icevision.visualize.wandb_img import * wandb_images = wandb_img_preds ( samples , preds , class_map , add_ground_truth = True ) wandb . log ({ \"Predicted images\" : wandb_images }) # optional: mark the run as completed wandb . join () Happy Learning! If you need any assistance, feel free to join our forum .","title":"Model Tracking Using Wandb"},{"location":"wandb_efficientdet/#icevision-meets-wb","text":"IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking For more information check the following Report IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. In this example, we are using the fastai training loop, it offers a slick integration with wandb through the use of the WandbCallback() callback.","title":"IceVision meets W&amp;B"},{"location":"wandb_efficientdet/#introduction","text":"In this tutorial, we walk you through the different steps of training the fridge dataset. Thanks to W&B, we can easily track the performance of the EfficientDet model using different backbones. In this example, we are using the fastai library training loop.","title":"Introduction"},{"location":"wandb_efficientdet/#installing-icevision-and-icedata","text":"! pip install icevision [ all ] icedata","title":"Installing IceVision and IceData"},{"location":"wandb_efficientdet/#imports","text":"from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback","title":"Imports"},{"location":"wandb_efficientdet/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse ()","title":"Datasets : Fridge Objects dataset"},{"location":"wandb_efficientdet/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"wandb_efficientdet/#train-and-validation-dataset-transforms","text":"# Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"wandb_efficientdet/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"wandb_efficientdet/#dataloader","text":"# DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"wandb_efficientdet/#model","text":"# EfficientDet D2 model = efficientdet . model ( 'tf_efficientdet_d2' , num_classes = len ( class_map ), img_size = 384 )","title":"Model"},{"location":"wandb_efficientdet/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"wandb_efficientdet/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"wandb_efficientdet/#training-using-fastai","text":"wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_d2-1\" , reinit = True ) <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc Tracking run with wandb version 0.10.8 Syncing run efficientdet_d2-1 to Weights & Biases (Documentation) . Project page: https://wandb.ai/ai-fast-track/icevision-fridge Run page: https://wandb.ai/ai-fast-track/icevision-fridge/runs/m4mz59fz Run data is saved locally in wandb/run-20201027_134038-m4mz59fz","title":"Training using fastai"},{"location":"wandb_efficientdet/#show-results","text":"efficientdet . show_results ( model , valid_ds , class_map = class_map )","title":"Show results"},{"location":"wandb_efficientdet/#inference","text":"infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model = model , infer_dl = infer_dl ) # from icevision.visualize.wandb_img import * wandb_images = wandb_img_preds ( samples , preds , class_map , add_ground_truth = True ) wandb . log ({ \"Predicted images\" : wandb_images }) # optional: mark the run as completed wandb . join ()","title":"Inference"},{"location":"wandb_efficientdet/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"wandb_masks/","text":"! nvidia - smi How to use Mask RCNN Installing IceVision We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. ! pip install icevision [ all ] icedata Imports from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback Data We'll be using the Penn-Fudan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=53723336.0), HTML(value=''))) As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m train_records [ 0 ][ \"masks\" ] <EncodedRLEs with 1 objects> Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 2 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) Metrics Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)] Model Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178090079.0), HTML(value=''))) wandb . init ( project = \"icevision-masks\" , name = \"mask-rcnn-resnet50-2\" , reinit = True ) Finishing last run (ID:eu733uxj) before initializing another... Waiting for W&B process to finish, PID 159 Program ended successfully. VBox(children=(Label(value=' 347.34MB of 347.34MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=\u2026 Find user logs for this run at: /content/wandb/run-20201112_010811-eu733uxj/logs/debug.log Find internal logs for this run at: /content/wandb/run-20201112_010811-eu733uxj/logs/debug-internal.log Run summary: table.wandb td:nth-child(1) { padding: 0 10px; text-align: right } epoch 12 train_loss 0.38875 raw_loss 0.38006 wd_0 0.01 sqr_mom_0 0.99 lr_0 0.0 mom_0 0.94994 eps_0 1e-05 wd_1 0.01 sqr_mom_1 0.99 lr_1 0.0 mom_1 0.94994 eps_1 1e-05 wd_2 0.01 sqr_mom_2 0.99 lr_2 0.0 mom_2 0.94994 eps_2 1e-05 wd_3 0.01 sqr_mom_3 0.99 lr_3 0.0 mom_3 0.94994 eps_3 1e-05 wd_4 0.01 sqr_mom_4 0.99 lr_4 0.0 mom_4 0.94994 eps_4 1e-05 wd_5 0.01 sqr_mom_5 0.99 lr_5 0.0 mom_5 0.94994 eps_5 1e-05 wd_6 0.01 sqr_mom_6 0.99 lr_6 0.0 mom_6 0.94994 eps_6 1e-05 wd_7 0.01 sqr_mom_7 0.99 lr_7 0.0 mom_7 0.94994 eps_7 1e-05 _step 109 _runtime 5124 _timestamp 1605148415 valid_loss 0.31662 Run history: table.wandb td:nth-child(1) { padding: 0 10px; text-align: right } epoch \u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588 train_loss \u2588\u2587\u2586\u2585\u2584\u2584\u2583\u2582\u2582\u2582\u2581\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 raw_loss \u2588\u2586\u2583\u2583\u2582\u2582\u2585\u2582\u2581\u2581\u2581\u2582\u2581\u2581\u2581\u2581\u2581\u2582\u2581\u2581\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2581\u2581\u2581\u2581 wd_0 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_0 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_0 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 mom_0 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_0 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_1 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_1 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_1 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 mom_1 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_1 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_2 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_2 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_2 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 mom_2 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_2 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_3 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_3 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_3 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2583\u2583\u2583\u2583\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581 mom_3 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_3 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_4 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_4 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_4 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2582\u2582\u2582\u2583\u2584\u2584\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2586\u2586\u2585\u2585\u2585\u2585\u2584\u2584\u2584\u2583\u2583\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581 mom_4 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_4 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_5 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_5 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_5 \u2581\u2581\u2582\u2583\u2584\u2585\u2586\u2582\u2583\u2583\u2584\u2585\u2585\u2586\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2586\u2586\u2585\u2585\u2585\u2584\u2584\u2583\u2583\u2582\u2582\u2582\u2581\u2581\u2581\u2581 mom_5 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_5 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_6 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_6 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_6 \u2581\u2581\u2582\u2582\u2583\u2583\u2584\u2582\u2583\u2583\u2584\u2585\u2585\u2586\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2586\u2586\u2585\u2585\u2585\u2584\u2584\u2583\u2583\u2582\u2582\u2582\u2581\u2581\u2581\u2581 mom_6 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_6 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 wd_7 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 sqr_mom_7 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 lr_7 \u2581\u2581\u2583\u2584\u2585\u2587\u2588\u2582\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2585\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2583\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581 mom_7 \u2588\u2588\u2587\u2585\u2584\u2582\u2581\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2587\u2587\u2587\u2588\u2588\u2588\u2588 eps_7 \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 _step \u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588 _runtime \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2588 _timestamp \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2588 valid_loss \u2588\u2585\u2583\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581 Synced 5 W&B file(s), 340 media file(s), 2 artifact file(s) and 0 other file(s) Synced mask-rcnn-resnet50-1 : https://wandb.ai/ai-fast-track/icevision-masks/runs/eu733uxj ...Successfully finished last run (ID:eu733uxj). Initializing new run: Tracking run with wandb version 0.10.10 Syncing run mask-rcnn-resnet50-2 to Weights & Biases (Documentation) . Project page: https://wandb.ai/ai-fast-track/icevision-masks Run page: https://wandb.ai/ai-fast-track/icevision-masks/runs/1m79qv7k Run data is saved locally in /content/wandb/run-20201112_023439-1m79qv7k Run(1m79qv7k) Training - fastai We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ WandbCallback ( log_dataset = True , log_model = True ), SaveModelCallback ()]) # !cp /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth models/ # !ls models # !rm models/model.pth learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mWandb quickfix implemented, for more info check issue #527\u001b[0m | \u001b[36micevision.models.rcnn.fastai.learner\u001b[0m:\u001b[36mrcnn_learner\u001b[0m:\u001b[36m39\u001b[0m Could not gather input dimensions WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\" WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss time 0 1.843482 0.828755 00:13 1 1.284275 0.588426 00:10 Better model found at epoch 0 with valid_loss value: 0.828754723072052. Better model found at epoch 1 with valid_loss value: 0.5884255766868591. /usr/local/lib/python3.6/dist-packages/fastai/learner.py:54: UserWarning: Saved filed doesn't contain an optimizer state. elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\") \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/tmpgcqxjvrk)... Done. 0.4s Could not gather input dimensions epoch train_loss valid_loss time 0 0.591160 0.429428 00:14 1 0.529282 0.374389 00:12 2 0.500065 0.406371 00:10 3 0.480722 0.368016 00:11 4 0.465826 0.340654 00:11 5 0.446898 0.317662 00:12 6 0.429577 0.323954 00:11 7 0.417124 0.314693 00:11 8 0.405281 0.313216 00:11 9 0.388749 0.316623 00:11 Exception ignored in: <finalize object at 0x7fdfac6a4190; dead> Traceback (most recent call last): File \"/usr/lib/python3.6/weakref.py\", line 548, in __call__ return info.func(*info.args, **(info.kwargs or {})) File \"/usr/lib/python3.6/tempfile.py\", line 938, in _cleanup _rmtree(name) File \"/usr/lib/python3.6/shutil.py\", line 477, in rmtree onerror(os.lstat, path, sys.exc_info()) File \"/usr/lib/python3.6/shutil.py\", line 475, in rmtree orig_st = os.lstat(path) FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpgcqxjvrk' Better model found at epoch 0 with valid_loss value: 0.42942774295806885. Better model found at epoch 1 with valid_loss value: 0.37438860535621643. Better model found at epoch 3 with valid_loss value: 0.36801600456237793. Better model found at epoch 4 with valid_loss value: 0.3406542241573334. Better model found at epoch 5 with valid_loss value: 0.31766191124916077. Better model found at epoch 7 with valid_loss value: 0.314692884683609. Better model found at epoch 8 with valid_loss value: 0.3132163882255554. \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/tmpf05n03p9)... Done. 0.4s Visualize predictions Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map ) infer_dl = mask_rcnn . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = mask_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value=''))) wandb_images = wandb_img_preds ( samples , preds , class_map , add_ground_truth = True ) len ( wandb_images ) wandb . log ({ \"Predicted images\" : wandb_images }) wandb . join () 34 Happy Learning! If you need any assistance, feel free to join our forum .","title":"Wandb masks"},{"location":"wandb_masks/#how-to-use-mask-rcnn","text":"","title":"How to use Mask RCNN"},{"location":"wandb_masks/#installing-icevision","text":"We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. ! pip install icevision [ all ] icedata","title":"Installing IceVision"},{"location":"wandb_masks/#imports","text":"from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback","title":"Imports"},{"location":"wandb_masks/#data","text":"We'll be using the Penn-Fudan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=53723336.0), HTML(value=''))) As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170.0), HTML(value=''))) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m124\u001b[0m train_records [ 0 ][ \"masks\" ] <EncodedRLEs with 1 objects> Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 2 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 )","title":"Data"},{"location":"wandb_masks/#metrics","text":"Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)]","title":"Metrics"},{"location":"wandb_masks/#model","text":"Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map )) Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178090079.0), HTML(value=''))) wandb . init ( project = \"icevision-masks\" , name = \"mask-rcnn-resnet50-2\" , reinit = True ) Finishing last run (ID:eu733uxj) before initializing another... Waiting for W&B process to finish, PID 159 Program ended successfully. VBox(children=(Label(value=' 347.34MB of 347.34MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=\u2026 Find user logs for this run at: /content/wandb/run-20201112_010811-eu733uxj/logs/debug.log Find internal logs for this run at: /content/wandb/run-20201112_010811-eu733uxj/logs/debug-internal.log","title":"Model"},{"location":"wandb_masks/#training-fastai","text":"We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ WandbCallback ( log_dataset = True , log_model = True ), SaveModelCallback ()]) # !cp /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth models/ # !ls models # !rm models/model.pth learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mWandb quickfix implemented, for more info check issue #527\u001b[0m | \u001b[36micevision.models.rcnn.fastai.learner\u001b[0m:\u001b[36mrcnn_learner\u001b[0m:\u001b[36m39\u001b[0m Could not gather input dimensions WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\" WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss time 0 1.843482 0.828755 00:13 1 1.284275 0.588426 00:10 Better model found at epoch 0 with valid_loss value: 0.828754723072052. Better model found at epoch 1 with valid_loss value: 0.5884255766868591. /usr/local/lib/python3.6/dist-packages/fastai/learner.py:54: UserWarning: Saved filed doesn't contain an optimizer state. elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\") \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/tmpgcqxjvrk)... Done. 0.4s Could not gather input dimensions epoch train_loss valid_loss time 0 0.591160 0.429428 00:14 1 0.529282 0.374389 00:12 2 0.500065 0.406371 00:10 3 0.480722 0.368016 00:11 4 0.465826 0.340654 00:11 5 0.446898 0.317662 00:12 6 0.429577 0.323954 00:11 7 0.417124 0.314693 00:11 8 0.405281 0.313216 00:11 9 0.388749 0.316623 00:11 Exception ignored in: <finalize object at 0x7fdfac6a4190; dead> Traceback (most recent call last): File \"/usr/lib/python3.6/weakref.py\", line 548, in __call__ return info.func(*info.args, **(info.kwargs or {})) File \"/usr/lib/python3.6/tempfile.py\", line 938, in _cleanup _rmtree(name) File \"/usr/lib/python3.6/shutil.py\", line 477, in rmtree onerror(os.lstat, path, sys.exc_info()) File \"/usr/lib/python3.6/shutil.py\", line 475, in rmtree orig_st = os.lstat(path) FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpgcqxjvrk' Better model found at epoch 0 with valid_loss value: 0.42942774295806885. Better model found at epoch 1 with valid_loss value: 0.37438860535621643. Better model found at epoch 3 with valid_loss value: 0.36801600456237793. Better model found at epoch 4 with valid_loss value: 0.3406542241573334. Better model found at epoch 5 with valid_loss value: 0.31766191124916077. Better model found at epoch 7 with valid_loss value: 0.314692884683609. Better model found at epoch 8 with valid_loss value: 0.3132163882255554. \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/tmpf05n03p9)... Done. 0.4s","title":"Training - fastai"},{"location":"wandb_masks/#visualize-predictions","text":"Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map ) infer_dl = mask_rcnn . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = mask_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value=''))) wandb_images = wandb_img_preds ( samples , preds , class_map , add_ground_truth = True ) len ( wandb_images ) wandb . log ({ \"Predicted images\" : wandb_images }) wandb . join () 34","title":"Visualize predictions"},{"location":"wandb_masks/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"examples/backbones_faster_rcnn/","text":"Using different Faster RCNN backbones In this example, we are training the Raccoon dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # fastai Learner learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Train using fastai fine tuning learn . fine_tune ( 20 , lr = 1e-4 ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Backbones - Faster RCNN"},{"location":"examples/backbones_faster_rcnn/#using-different-faster-rcnn-backbones","text":"In this example, we are training the Raccoon dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # fastai Learner learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Train using fastai fine tuning learn . fine_tune ( 20 , lr = 1e-4 ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Using different Faster RCNN backbones"},{"location":"examples/dataset_voc_exp/","text":"How to train a VOC compatible dataset This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating data, and parsers files as opposed to the fridge dataset example. Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Fine tune: 2 Phases # Phase 1: Train the head for 10 epochs while freezing the body # Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 60 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Training a VOC dataset"},{"location":"examples/dataset_voc_exp/#how-to-train-a-voc-compatible-dataset","text":"This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating data, and parsers files as opposed to the fridge dataset example. Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Fine tune: 2 Phases # Phase 1: Train the head for 10 epochs while freezing the body # Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 60 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"How to train a VOC compatible dataset"},{"location":"examples/efficientdet_pets_exp/","text":"How to use EffecientDet In this example, we show how to train an EffecientDet model on the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 # For EffecientDet the size of the image has to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training learn . freeze () learn . lr_find () learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 # EffecientDet requires the image size to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"EffecientDet"},{"location":"examples/efficientdet_pets_exp/#how-to-use-effecientdet","text":"In this example, we show how to train an EffecientDet model on the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 # For EffecientDet the size of the image has to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training learn . freeze () learn . lr_find () learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 # EffecientDet requires the image size to be divisible by 128 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"How to use EffecientDet"},{"location":"examples/getting_started_exp/","text":"Training and End-to-End dataset (PETS) In this example, we are training the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using fastai2 learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , lr = 1e-4 ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse () # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Getting Started"},{"location":"examples/getting_started_exp/#training-and-end-to-end-dataset-pets","text":"In this example, we are training the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using fastai2 learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , lr = 1e-4 ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse () # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training and End-to-End dataset (PETS)"},{"location":"examples/inference_exp/","text":"How to use the inference API The inference API is unified one. It is independent from both Fastai or Pytorch-Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Maps IDs to class names. `print(class_map)` for all available classes class_map = icedata . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"Inference"},{"location":"examples/inference_exp/#how-to-use-the-inference-api","text":"The inference API is unified one. It is independent from both Fastai or Pytorch-Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Maps IDs to class names. `print(class_map)` for all available classes class_map = icedata . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"How to use the inference API"},{"location":"examples/mask_rcnn_pennfudan_exp/","text":"Using Mask RCNN This shows how to train a MaskRCNN model on the Penn-Fundan dataset using either Fastai or Pytorch-Lightning training loop. Fastai # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) # Parse records with random splits train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Create Fastai Learner and train the model learn = mask_rcnn . fastai . learner ( model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 ) Pytorch Lightning # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Mask RCNN"},{"location":"examples/mask_rcnn_pennfudan_exp/#using-mask-rcnn","text":"This shows how to train a MaskRCNN model on the Penn-Fundan dataset using either Fastai or Pytorch-Lightning training loop. Fastai # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) # Parse records with random splits train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Create Fastai Learner and train the model learn = mask_rcnn . fastai . learner ( model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 ) Pytorch Lightning # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Using Mask RCNN"},{"location":"examples/quickstart_exp/","text":"Training and End-to-End dataset (Fridge Objects) In this example, we are training the Fridge Objects dataset using either Fastai or Pytorch-Lightning training loop Fastai # pip install icevision[all] icedata from icevision.all import * url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations/\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # pip install icevision[all] icedata from icevision.all import * import icedata url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Pytorch Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 70 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Quickstart"},{"location":"examples/quickstart_exp/#training-and-end-to-end-dataset-fridge-objects","text":"In this example, we are training the Fridge Objects dataset using either Fastai or Pytorch-Lightning training loop Fastai # pip install icevision[all] icedata from icevision.all import * url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations/\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # pip install icevision[all] icedata from icevision.all import * import icedata url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Pytorch Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 70 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Training and End-to-End dataset (Fridge Objects)"}]}